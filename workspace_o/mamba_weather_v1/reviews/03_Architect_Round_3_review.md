# ğŸ—ï¸ æ·±åº¦æ¶æ„è“å›¾: 03_Architect_Round_3

## 1. ç³»ç»Ÿæ¦‚è§ˆ
- **Project**: `TimeMixer`
- **Data Flow**: 
```text
Data -> [Batch, Time, Features] -> [PDM Block] -> [Seasonal, Trend] -> [FMM Block] -> Output [Batch, Time, Predictions]
2. æ ¸å¿ƒæ–‡ä»¶ç»“æ„ (Detailed Specs)


ğŸ“„ æ–‡ä»¶: src\data\preprocessing.py

Data preprocessing for time series.
Imports: numpy as np, pandas as pd



ğŸ”§ Functions


ğŸ”¹ decompose_time_series

Logic: Apply seasonal decomposition on the data. -> Extract seasonal component. -> Extract trend component. -> Return seasonal and trend components as numpy arrays.


ğŸ“„ æ–‡ä»¶: src\models\time_mixer.py

Implementation of the TimeMixer architecture.
Imports: torch, torch.nn as nn


ğŸ“¦ Classes


Class TimeMixer (inherits nn.Module)

Attrs: num_scales: int, num_layers: int, seasonal_layer: nn.Module, trend_layer: nn.Module, predictors: List[nn.Module]

Methods:

ğŸ”¹ __init__(self, num_scales: int, num_layers: int) -> None

ğŸ“ Initialize the TimeMixer model with given scales and layers.

âš™ï¸ Logic:

Set self.num_scales to num_scales.

Set self.num_layers to num_layers.

Initialize seasonal_layer as an instance of nn.Module.

Initialize trend_layer as an instance of nn.Module.

Initialize predictors as an empty list.

For each scale in range(num_scales):

    Append a new predictor module to self.predictors.

ğŸ”¹ forward(self, x: torch.Tensor) -> torch.Tensor

ğŸ“ Perform forward pass through the TimeMixer model.

âš™ï¸ Logic:

Input x has shape [Batch, Time, Features].

Decompose x into seasonal and trend components using seasonal_layer and trend_layer.

For each scale in range(self.num_scales):

    Compute predictions from each predictor on seasonal and trend components.

Aggregate the predictions from all predictors.

Return the aggregated predictions.



ğŸ“„ æ–‡ä»¶: src\training\train.py

Training loop for the TimeMixer model.
Imports: torch, torch.optim as optim, torch.nn.functional as F, src.models.time_mixer.TimeMixer



ğŸ”§ Functions


ğŸ”¹ train_model

Logic: Set optimizer as optim.Adam for the model parameters with learning rate. -> For epoch in range(num_epochs): ->     For batch in train_loader: ->         Get input data x and target y from batch. ->         Zero the gradients. ->         Compute model output by passing x through model. ->         Compute loss using F.mse_loss between output and target. ->         Backpropagate the loss. ->         Update model parameters using optimizer.


3. é…ç½®ä¸ä¾èµ–

Hyperparams: {'num_scales': '3', 'num_layers': '2', 'learning_rate': '0.001', 'num_epochs': '50'}

Requirements: torch==1.12.0, numpy==1.21.0, pandas==1.3.0

<!-- SYSTEM SEPARATOR -->

ğŸŸ¢ ç”¨æˆ·å†³ç­–åŒº

å†³ç­– (Action): [ APPROVE ]

åé¦ˆæ„è§ (Feedback):

<!-- æ¯”å¦‚ï¼šforward å‡½æ•°é‡Œçš„ shape å¥½åƒä¸å¯¹ï¼Œåº”è¯¥æ˜¯ [B, D, L] -->