# ğŸ§  ç†è®ºæ¡†æ¶å®¡æŸ¥: 02_Theory_Round_3

## 1. ç ”ç©¶é¢†åŸŸ (Field)
`Deep Learning`

## 2. é—®é¢˜å®šä¹‰ (Problem Formulation)
> è¿™é‡Œæ˜¯é—®é¢˜çš„å½¢å¼åŒ–å®šä¹‰ï¼ˆæ”¯æŒ LaTeXï¼‰ï¼š

Given a time series dataset \{x_1, x_2, \ldots, x_n\} with intricate temporal variations, our objective is to develop a model that effectively captures both short-range and long-range dependencies to improve forecasting accuracy. We aim to formulate the problem as an optimization task: \min_{\theta} L(y, f(x; \theta)), where \theta represents the model parameters, y is the true output, and f(x; \theta) is the model prediction for input x.

## 3. æ ¸å¿ƒæ–¹æ³• (Proposed Methodology)
We introduce TimeMixer, a multiscale mixing architecture. The architecture consists of two main components: Past-Decomposable-Mixing (PDM) and Future-Multipredictor-Mixing (FMM). The PDM block decomposes the input time series into seasonal and trend components via a series decomposition process, then employs both bottom-up and top-down mixing strategies to aggregate information across multiple scales. The FMM block utilizes multiple predictors to ensemble the forecasts from different scales, effectively leveraging the complementary information from these predictors to enhance the final predictions.

## 4. ç†è®ºåˆ†æ (Analysis & Proofs)
The complexity of the proposed TimeMixer architecture is analyzed as follows: Let M be the number of scales and N be the dimensionality of the model. The computational complexity for each PDM layer is O(M \times N^2) due to the mixing operations on seasonal and trend components. The FMM block operates with complexity O(M \times N), leading to an overall complexity of O(L \times M \times N^2) for L layers. This subquadratic complexity demonstrates the efficiency of our architecture compared to traditional Transformers, which exhibit quadratic complexity.

## 5. å…³é”®åˆ›æ–°ç‚¹ (Innovations)
- Introduces a multiscale mixing architecture that effectively disentangles intricate temporal variations by leveraging seasonal and trend components.
- Employs a novel Past-Decomposable-Mixing (PDM) block to aggregate information from multiple scales, enhancing the model's ability to capture both short-range and long-range dependencies.
- Implements a Future-Multipredictor-Mixing (FMM) block to ensemble predictions from different scales, thereby improving the robustness of the forecasting task.
- Achieves subquadratic complexity in both computation and memory usage, making it a more efficient alternative to traditional Transformer architectures.
- Demonstrates state-of-the-art performance across various forecasting tasks, showcasing the model's generalizability and effectiveness in handling complex temporal patterns.

---
<!-- SYSTEM SEPARATOR -->

# ğŸŸ¢ ç”¨æˆ·å†³ç­–åŒº

**å†³ç­– (Action)**: [ APPROVE ] 

**åé¦ˆæ„è§ (Feedback)**:
<!-- æ¯”å¦‚ï¼šå…¬å¼æ¨å¯¼æœ‰ç‚¹é—®é¢˜ï¼Œæˆ–è€…è¯·å¢åŠ å¯¹éšç§å®‰å…¨æ€§çš„è®¨è®º -->