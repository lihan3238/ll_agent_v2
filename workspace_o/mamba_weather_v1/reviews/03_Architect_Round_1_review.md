# ğŸ—ï¸ æ·±åº¦æ¶æ„è“å›¾: 03_Architect_Round_1

## 1. ç³»ç»Ÿæ¦‚è§ˆ
- **Project**: `TimeMixer`
- **Data Flow**: 
```text
Input Data -> [Batch, Time, Features] -> Model (TimeMixer) -> Output Data [Batch, Time, Predictions]
2. æ ¸å¿ƒæ–‡ä»¶ç»“æ„ (Detailed Specs)


ğŸ“„ æ–‡ä»¶: src\data\data_preprocessing.py

Handles data loading and preprocessing.
Imports: import pandas as pd, import numpy as np



ğŸ”§ Functions


ğŸ”¹ load_data

Logic: 1. Use pd.read_csv(file_path) to read the data. -> 2. Return the loaded DataFrame.

ğŸ”¹ preprocess_data

Logic: 1. Normalize the data using Min-Max scaling. -> 2. Convert the DataFrame to a NumPy array. -> 3. Return the processed data.


ğŸ“„ æ–‡ä»¶: src\models\time_mixer.py

Defines the TimeMixer model architecture.
Imports: import torch, import torch.nn as nn


ğŸ“¦ Classes


Class TimeMixer (inherits nn.Module)

Attrs: num_scales: int, input_dim: int, pdm_layers: nn.ModuleList, fmm_layer: nn.Linear

Methods:

ğŸ”¹ __init__(self, num_scales: int, input_dim: int) -> None

ğŸ“ Initializes the TimeMixer model with specified scales and input dimension.

âš™ï¸ Logic:

1. Call super().__init__() to initialize the parent class.

2. Set self.num_scales to num_scales.

3. Set self.input_dim to input_dim.

4. Initialize self.pdm_layers as nn.ModuleList of PDM layers.

5. Initialize self.fmm_layer as nn.Linear with input dimension and output dimension.

ğŸ”¹ forward(self, x: torch.Tensor) -> torch.Tensor

ğŸ“ Computes the forward pass of the TimeMixer model.

âš™ï¸ Logic:

1. Initialize an empty list to store outputs from PDM layers.

2. For each layer in self.pdm_layers:

   a. Apply layer to the input x.

   b. Append the output to the outputs list.

3. Concatenate outputs from all PDM layers along the last dimension.

4. Pass the concatenated outputs to self.fmm_layer to obtain final predictions.

5. Return the final predictions.



ğŸ“„ æ–‡ä»¶: src\training\train.py

Contains the training loop for the TimeMixer model.
Imports: import torch, import torch.optim as optim, from src.models.time_mixer import TimeMixer, from src.data.data_preprocessing import load_data, preprocess_data



ğŸ”§ Functions


ğŸ”¹ train_model

Logic: 1. Set the model to training mode: model.train(). -> 2. Create an optimizer using optim.Adam with model parameters and learning_rate. -> 3. For each epoch in range(epochs): ->    a. For each batch in train_loader: ->       i. Zero the gradients: optimizer.zero_grad(). ->       ii. Forward pass: get predictions from model using batch data. ->       iii. Calculate loss using a loss function (e.g., MSELoss). ->       iv. Backward pass: loss.backward(). ->       v. Update model parameters: optimizer.step().


3. é…ç½®ä¸ä¾èµ–

Hyperparams: {'num_scales': '3', 'input_dim': '10', 'epochs': '50', 'learning_rate': '0.001'}

Requirements: torch==1.10.0, numpy==1.21.0

<!-- SYSTEM SEPARATOR -->

ğŸŸ¢ ç”¨æˆ·å†³ç­–åŒº

å†³ç­– (Action): [ APPROVE ]

åé¦ˆæ„è§ (Feedback):

<!-- æ¯”å¦‚ï¼šforward å‡½æ•°é‡Œçš„ shape å¥½åƒä¸å¯¹ï¼Œåº”è¯¥æ˜¯ [B, D, L] -->