llm:
  default_base_url: null # 让代码自动去读 DEEPSEEK_BASE_URL
  default_model: "DeepSeek-V3.2"
  # [新增] 全局默认输出长度限制
  default_max_tokens: 16384

agents:
  researcher:
    model: "DeepSeek-V3.2" # 比如用 deepseek 读论文便宜
    temperature: 0.7
    max_tokens: 8192 # [覆盖] 允许输出更多内容
    
    # [新增] 自定义流程参数
    query_count: 3                # 生成多少个搜索关键词
    search_limit_per_query: 3     # 每个关键词搜几篇
    max_context_papers: 5         # 最终给 LLM 看几篇最相关的 (防止上下文溢出)
    timeout: 15                   # API 超时时间

  theorist:
    model: "DeepSeek-V3.2" # 聪明，负责逻辑推理
    temperature: 0.7
    max_tokens: 8192 # [覆盖] 允许输出更多内容

  architect:
    model: "DeepSeek-V3.2"
    temperature: 0.5
    max_tokens: 16384 # [覆盖] 允许输出更多内容

  # [新增] 论文写手
  paper_writer:
    model: "DeepSeek-V3.2" # 写作用 V3 足够，速度快
    temperature: 0.4       # 稍低温度，保证学术规范
    max_tokens: 65536      # 写长文需要大窗口

  # [新增] 程序员
  coder:
    model: "DeepSeek-V3.2" # 写代码 V3 性价比最高，R1 也可以
    temperature: 0.2           # 代码生成需要低温度，严谨
    max_tokens: 65536

  # [新增] 评审专家
  reviewer:
    model: "DeepSeek-V3.2" # 必须聪明且严谨
    temperature: 0.2 # 低温，减少幻觉，保持严厉
    max_retries: 3
    max_tokens: 4196 # [覆盖] 够写评语就行，省钱

  translator:
    model: "DeepSeek-V3.2" # 翻译需要稍微好一点的模型理解语意，DeepSeek 也可以
    temperature: 0.1
    max_tokens: 8192

project:
  # [决定权在你]: 这里填什么，就会生成 workspace/mamba_weather_v3/ 文件夹
  # name: "mamba_weather_v3" 
  # name: "ai_for_building_energy_consumption_forecasting_v0" 
  name: "Towards_Public_Verifiable_Encrypted_Search_by_Using_Blockchain_and_CBF_v1"
  # 模式: interactive (有人值守) / autonomous (无人挂机)
  # mode: "interactive"
  mode: "autonomous"
  # [新增] 用户偏好语言 (用于生成 Markdown 里的提示语，但不影响内核)
  user_language: "zh"

  latex_template: "blank_icml_latex" #

workflow:
  # 每个阶段的 Review 轮次 (大循环)
  research_rounds: 3
  theory_rounds: 3
  architect_rounds: 2
  paper_rounds: 2
  coder_retries: 5

  # 每次 Review 前 Agent 自己闷头跑几轮 (小循环)
  internal_loops: 2

# [新增] 运行环境硬性约束 (Coder 会读取这个来生成 environment.yaml)
execution_env:
  os_platform: "linux"  
  # 硬件信息 (提示 LLM 生成正确的 CUDA 依赖)
  experience_context: "wsl-ubuntu CUDA Driver Version: 13.0"
  python_version: "3.11" # 推荐 3.11，性能更好
  # 强制依赖 (Base libs)
  base_requirements:
    conda:
      - "numpy"
      - "pandas"
      - "matplotlib"
      - "scikit-learn"
    pip:
      - "--extra-index-url https://download.pytorch.org/whl/cu130"
      - "torch"
      - "torchvision"
      - "torchaudio"