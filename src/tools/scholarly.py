# src/tools/scholarly.py
import requests
import os
import time
from typing import List, Dict
from src.utils.logger import sys_logger

class SemanticScholarTool:
    def __init__(self):
        self.api_key = os.getenv("S2_API_KEY")
        self.base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
        self.headers = {}
        if self.api_key:
            self.headers["x-api-key"] = self.api_key

    def search(self, query: str, limit: int = 5) -> List[Dict]:
        """
        æ ¹æ® query æœç´¢è®ºæ–‡ï¼Œè¿”å›æ¸…æ´—åçš„åˆ—è¡¨
        """
        sys_logger.info(f"Checking Semantic Scholar for: '{query}'")
        time.sleep(3)

        params = {
            "query": query,
            "limit": limit,
            # æŒ‡å®šæˆ‘ä»¬éœ€è¦è¿”å›çš„å­—æ®µï¼ŒèŠ‚çœå¸¦å®½å’ŒToken
            "fields": "title,abstract,year,citationCount,authors,url"
        }

        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                sys_logger.info(f"ğŸŒ S2 API Request (Attempt {attempt+1}/{max_retries}): '{query}'")
                
                response = requests.get(
                    self.base_url, 
                    params=params, 
                    headers=self.headers, 
                    timeout=15 # ç¨å¾®å¢åŠ è¶…æ—¶æ—¶é—´
                )
                
                # Case A: æˆåŠŸ
                if response.status_code == 200:
                    data = response.json()
                    papers = data.get("data", [])
                    if not papers:
                        sys_logger.warning(f"No papers found for query: {query}")
                        return []
                    
                    # æ¸…æ´—æ•°æ®
                    cleaned_papers = []
                    for p in papers:
                        if not p.get("abstract"):
                            continue
                        cleaned_papers.append({
                            "title": p.get("title", "Unknown Title"),
                            "year": p.get("year", "N/A"),
                            "citations": p.get("citationCount", 0),
                            "abstract": p.get("abstract", "").replace("\n", " "),
                            "url": p.get("url", ""),
                            "authors": ", ".join([a["name"] for a in p.get("authors", [])][:3])
                        })
                    return cleaned_papers
                
                # Case B: éœ€è¦é‡è¯•çš„é”™è¯¯ (429 Too Many Requests, 5xx Server Error)
                elif response.status_code == 429 or response.status_code >= 500:
                    sys_logger.warning(f"âš ï¸ API Status {response.status_code}. Retrying in 5s...")
                    time.sleep(3)
                    continue # è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯
                
                # Case C: å®¢æˆ·ç«¯é”™è¯¯ (400 Bad Request ç­‰)ï¼Œé€šå¸¸æ˜¯å› ä¸º Query æ ¼å¼ä¸å¯¹ï¼Œé‡è¯•æ²¡ç”¨
                else:
                    sys_logger.error(f"âŒ S2 API Error {response.status_code}: {response.text}")
                    return []

            except requests.exceptions.RequestException as e:
                # ç½‘ç»œå±‚é¢æŠ¥é”™ (æ–­ç½‘ã€DNSè§£æå¤±è´¥ç­‰)
                sys_logger.warning(f"âš ï¸ Network Error: {e}. Retrying in 5s...")
                time.sleep(3)
                continue

        # å¦‚æœå¾ªç¯ç»“æŸè¿˜æ²¡è¿”å›ï¼Œè¯´æ˜å¤±è´¥äº†
        sys_logger.error(f"âŒ Failed to fetch papers for '{query}' after {max_retries} attempts.")
        return []

# å•ä¾‹å®ä¾‹
s2_tool = SemanticScholarTool()