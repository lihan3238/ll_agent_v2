# src/core/llm.py
import os
import httpx
from openai import OpenAI
from dotenv import load_dotenv
from src.utils.logger import llm_logger
import sys

load_dotenv()

def call_llm(
    prompt: str, 
    model: str, 
    base_url: str = None, 
    temperature: float = 0.7, 
    max_tokens: int = 16384,
    agent_name: str = "unknown"
) -> str:
    """
    ç»Ÿä¸€ LLM æ¥å£ (Streaming + å®æ—¶æ§åˆ¶å°è¾“å‡º + é²æ£’æ€§å¢å¼º)
    """
    # 1. é‰´æƒé€»è¾‘
    api_key = None
    final_base_url = base_url
    
    if "deepseek" in model.lower():
        if not final_base_url: final_base_url = os.getenv("DEEPSEEK_BASE_URL")
        api_key = os.getenv("DEEPSEEK_API_KEY")
    
    if not final_base_url: final_base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    if not api_key: api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key: 
        api_key = os.getenv("API_KEY") # æœ€åçš„å°è¯•
        if not api_key:
            raise ValueError(f"Missing API Key for model {model}")

    # ================= LOGGING (FILE) =================
    llm_logger.info(f"======== [REQUEST] Agent: {agent_name} | Model: {model} ========")
    llm_logger.info(f"URL: {final_base_url}")
    llm_logger.info(f"PROMPT HEAD (First 500 chars):\n{prompt[:500]}...") 
    llm_logger.info("-" * 50)
    # =================================================

    # è¶…æ—¶è®¾ç½®ï¼šArchitect ç”Ÿæˆé•¿æ–‡æœ¬éœ€è¦å¾ˆé•¿æ—¶é—´
    # read=600 æ„å‘³ç€å¦‚æœæœåŠ¡å™¨ 600ç§’ ä¸åå­—æ‰ç®—è¶…æ—¶
    timeout = httpx.Timeout(connect=15.0, read=600.0, write=15.0, pool=15.0)

    client = OpenAI(
        base_url=final_base_url, 
        api_key=api_key,
        timeout=timeout
    )

    try:
        kwargs = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": max_tokens,
            "stream": True # å¼ºåˆ¶æµå¼
        }
        
        # O1/Reasoning æ¨¡å‹å…¼å®¹æ€§
        if "o1" not in model.lower() and "reasoner" not in model.lower():
            kwargs["temperature"] = temperature

        response_stream = client.chat.completions.create(**kwargs)
        
        collected_content = []
        
        print(f"\nğŸ¤– [{agent_name}] Generating:", end="\n", flush=True)
        print("-" * 40) 
        
        chunk_count = 0
        
        for chunk in response_stream:
            chunk_count += 1
            delta = chunk.choices[0].delta
            
            # [å…¼å®¹æ€§ä¿®å¤] ä¼˜å…ˆè·å– contentï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°è¯•è·å– reasoning_content (é’ˆå¯¹ DeepSeek R1)
            # æ³¨æ„ï¼šæ ‡å‡† OpenAI åº“å¯èƒ½æ²¡æœ‰ reasoning_content å±æ€§ï¼Œéœ€ç”¨ getattr å®‰å…¨è·å–
            content = delta.content
            reasoning = getattr(delta, 'reasoning_content', None)
            
            # ä¼˜å…ˆä½¿ç”¨ contentï¼›å¦‚æœæ˜¯ R1 ä¸” content ä¸ºç©ºä½†æœ‰ reasoningï¼Œä¹Ÿå¯ä»¥æš‚æ—¶æ‰“å°å‡ºæ¥çœ‹çœ‹
            # ä½†æœ€ç»ˆæˆ‘ä»¬åªéœ€è¦ contentã€‚å¦‚æœæ¨¡å‹åªè¿”å› reasoningï¼Œè¯´æ˜ Prompt æ²¡å¼•å¯¼å®ƒè¾“å‡ºç»“è®ºã€‚
            
            part = content if content is not None else ""
            
            if part:
                collected_content.append(part)
                print(part, end="", flush=True)
            
            # å¦‚æœæ˜¯ DeepSeek R1 çš„æ€è€ƒè¿‡ç¨‹ï¼Œé€‰æ‹©æ€§æ‰“å°ï¼ˆå¯é€‰ï¼‰
            # if reasoning:
            #     print(f"[Think: {reasoning}]", end="", flush=True)

        print("\n" + "-" * 40)
        
        full_content = "".join(collected_content)
        
        # [æ ¸å¿ƒä¿®å¤] ç©ºå“åº”æ£€æŸ¥
        if not full_content.strip():
            err_msg = f"LLM returned empty response! (Chunks received: {chunk_count})"
            llm_logger.error(err_msg)
            # æ‰“å° Prompt å°¾éƒ¨ä»¥ä¾›è°ƒè¯•
            llm_logger.error(f"PROMPT TAIL:\n...{prompt[-500:]}")
            raise RuntimeError(err_msg)

        print(f"âœ… [{agent_name}] Generation Complete. Length: {len(full_content)}")
        
        # ================= LOGGING (FILE) =================
        llm_logger.info(f"======== [RESPONSE] Agent: {agent_name} ========")
        llm_logger.info(f"Total Length: {len(full_content)} chars")
        llm_logger.info(f"CONTENT HEAD:\n{full_content[:500]}...") 
        llm_logger.info("=" * 60 + "\n") 

        return full_content

    except Exception as e:
        llm_logger.error(f"LLM Call Failed for {agent_name}: {str(e)}")
        print(f"\nâŒ LLM Error: {str(e)}")
        raise RuntimeError(f"LLM API Call Failed: {str(e)}")