# üìù Èò∂ÊÆµÂÆ°Êü•: 01_Research_Round_2

## 1. Ê†∏ÂøÉÊÄùË∑Ø (Refined Idea)
> **This research introduces a physics-informed state space model framework that addresses the structural limitations of existing Mamba architectures for multivariate weather forecasting. We propose: (1) A novel parameterization of Mamba's time-varying system matrices (Œî, A, B, C) using atmospheric physics-informed constraints formalized through stochastic differential equations with weather-dependent transition kernels to handle regime changes; (2) Mathematical proofs establishing approximation bounds for Mamba's selection mechanism in capturing non-stationary atmospheric dynamics, overcoming the linear time-invariant assumption; (3) A rigorous benchmarking protocol against Transformers and existing Mamba variants using specialized metrics for regime transition detection and long-range dependency modeling, with comprehensive ablation studies isolating the contribution of selection mechanisms versus computational efficiency.**

## 2. ÊêúÁ¥¢ÂÖ≥ÈîÆËØç (Keywords)
- `physics-informed state space models`
- `non-stationary atmospheric dynamics`
- `stochastic differential equations`
- `regime transition detection`
- `multivariate weather forecasting`
- `Mamba architecture adaptation`
- `long-range dependency modeling`

## 3. Áõ∏ÂÖ≥Â∑•‰ΩúÁªºËø∞ (Related Work)
Current state-of-the-art in time series forecasting reveals significant limitations across architectural paradigms. Transformer-based approaches [2,5] demonstrate outstanding performance but suffer from quadratic complexity and ineffective position encoding for continuous temporal dynamics, particularly problematic for long-range weather dependencies. The Mamba architecture has shown promise in computer vision [3] and video generation [4] through efficient state space modeling, but these applications lack domain-specific adaptations for physical systems. Classical methods like ARIMA [1] remain mathematically simple but fundamentally limited by linear assumptions that cannot capture the nonlinear, non-stationary nature of atmospheric dynamics. Critically, existing Mamba implementations assume linear time-invariant systems, rendering them inadequate for weather forecasting where regime changes and non-stationary behavior are inherent. This creates a fundamental gap: no current method combines the computational efficiency of state space models with physics-informed constraints necessary for accurate atmospheric modeling.

## 4. ÈáçÁÇπËÆ∫Êñá (Top Papers)
| Year | Citations | Title |
| :---: | :---: | --- |
| 2023 | 207 | [A Review of ARIMA vs. Machine Learning Approaches for Time Series Forecasting in Data Driven Networks](https://www.semanticscholar.org/paper/1f1899907a1121e9c224a0cd01676e8ac7c03ff2) |
| 2023 | 129 | [Improving position encoding of transformers for multivariate time series classification](https://www.semanticscholar.org/paper/905a401efd96d4a5705f64b67cef486b43e83e20) |
| 2024 | 52 | [Mamba YOLO: A Simple Baseline for Object Detection with State Space Model](https://www.semanticscholar.org/paper/d5799c2c9feba526270d213404ee5b786da64d84) |
| 2024 | 27 | [Matten: Video Generation with Mamba-Attention](https://www.semanticscholar.org/paper/95fa89d968216b7c31e5993bd9bc08dfd9a81c20) |
| 2022 | 24 | [Unsupervised Multivariate Time-Series Transformers for Seizure Identification on EEG](https://www.semanticscholar.org/paper/172610ab93bc4bf3eb1b01c00ce31d17c1d73f1b) |

### ËÆ∫ÊñáÊëòË¶ÅËØ¶ÊÉÖ:
- **A Review of ARIMA vs. Machine Learning Approaches for Time Series Forecasting in Data Driven Networks**
  - *Summary*: In the broad scientific field of time series forecasting, the ARIMA models and their variants have been widely applied for half a century now due to their mathematical simplicity and flexibility in application. However, with the recent advances in the development and efficient deployment of artificial intelligence models and techniques, the view is rapidly changing, with a shift towards machine and deep learning approaches becoming apparent, even without a complete evaluation of the superiority of the new approach over the classic statistical algorithms. Our work constitutes an extensive review of the published scientific literature regarding the comparison of ARIMA and machine learning algorithms applied to time series forecasting problems, as well as the combination of these two approaches in hybrid statistical-AI models in a wide variety of data applications (finance, health, weather, utilities, and network traffic prediction). Our review has shown that the AI algorithms display better prediction performance in most applications, with a few notable exceptions analyzed in our Discussion and Conclusions sections, while the hybrid statistical-AI models steadily outperform their individual parts, utilizing the best algorithmic features of both worlds.
- **Improving position encoding of transformers for multivariate time series classification**
  - *Summary*: Transformers have demonstrated outstanding performance in many applications of deep learning. When applied to time series data, transformers require effective position encoding to capture the ordering of the time series data. The efficacy of position encoding in time series analysis is not well-studied and remains controversial, e.g., whether it is better to inject absolute position encoding or relative position encoding, or a combination of them. In order to clarify this, we first review existing absolute and relative position encoding methods when applied in time series classification. We then proposed a new absolute position encoding method dedicated to time series data called time Absolute Position Encoding (tAPE). Our new method incorporates the series length and input embedding dimension in absolute position encoding. Additionally, we propose computationally Efficient implementation of Relative Position Encoding (eRPE) to improve generalisability for time series. We then propose a novel multivariate time series classification model combining tAPE/eRPE and convolution-based input encoding named ConvTran to improve the position and data embedding of time series data. The proposed absolute and relative position encoding methods are simple and efficient. They can be easily integrated into transformer blocks and used for downstream tasks such as forecasting, extrinsic regression, and anomaly detection. Extensive experiments on 32 multivariate time-series datasets show that our model is significantly more accurate than state-of-the-art convolution and transformer-based models. Code and models are open-sourced at https://github.com/Navidfoumani/ConvTran .
- **Mamba YOLO: A Simple Baseline for Object Detection with State Space Model**
  - *Summary*: Driven by the rapid development of deep learning technology, the YOLO series has set a new benchmark for real-time object detectors. Additionally, transformer-based structures have emerged as the most powerful solution in the field, greatly extending the model's receptive field and achieving significant performance improvements. However, this improvement comes at a cost, as the quadratic complexity of the self-attentive mechanism increases the computational burden of the model. To address this problem, we introduce a simple yet effective baseline approach called Mamba YOLO. Our contributions are as follows: 1) We propose that the ODMamba backbone introduce a State Space Model (SSM) with linear complexity to address the quadratic complexity of self-attention. Unlike the other Transformer-base and SSM-base method, ODMamba is simple to train without pretraining. 2) For real-time requirement, we designed the macro structure of ODMamba, determined the optimal stage ratio and scaling size. 3) We design the RG Block that employs a multi-branch structure to model the channel dimensions, which addresses the possible limitations of SSM in sequence modeling, such as insufficient receptive fields and weak image localization. This design captures localized image dependencies more accurately and significantly. Extensive experiments on the publicly available COCO benchmark dataset show that Mamba YOLO achieves state-of-the-art performance compared to previous methods. Specifically, a tiny version of Mamba YOLO achieves a 7.5% improvement in mAP on a single 4090 GPU with an inference time of 1.5 ms.
- **Matten: Video Generation with Mamba-Attention**
  - *Summary*: In this paper, we introduce Matten, a cutting-edge latent diffusion model with Mamba-Attention architecture for video generation. With minimal computational cost, Matten employs spatial-temporal attention for local video content modeling and bidirectional Mamba for global video content modeling. Our comprehensive experimental evaluation demonstrates that Matten has competitive performance with the current Transformer-based and GAN-based models in benchmark performance, achieving superior FVD scores and efficiency. Additionally, we observe a direct positive correlation between the complexity of our designed model and the improvement in video quality, indicating the excellent scalability of Matten.
- **Unsupervised Multivariate Time-Series Transformers for Seizure Identification on EEG**
  - *Summary*: Epilepsy is one of the most common neurological disorders, typically observed via seizure episodes. Epileptic seizures are commonly monitored through electroencephalogram (EEG) recordings due to their routine and low expense collection. The stochastic nature of EEG makes seizure identification via manual inspections performed by highly-trained experts a tedious endeavor, motivating the use of automated identification. The literature on automated identification focuses mostly on supervised learning methods requiring expert labels of EEG segments that contain seizures, which are difficult to obtain. Motivated by these observations, we pose seizure identification as an unsupervised anomaly detection problem. To this end, we employ the first unsupervised transformer-based model for seizure identification on raw EEG. We train an autoencoder involving a transformer encoder via an unsupervised loss function, incorporating a novel masking strategy uniquely designed for multivariate time-series data such as EEG. Training employs EEG recordings that do not contain any seizures, while seizures are identified with respect to reconstruction errors at inference time. We evaluate our method on three publicly available benchmark EEG datasets for distinguishing seizure vs. non-seizure windows. Our method leads to significantly better seizure identification performance than supervised learning counterparts, by up to 16% recall, 9% accuracy, and 9% Area under the Receiver Operating Characteristics Curve (AUC), establishing particular benefits on highly imbalanced data. Through accurate seizure identification, our method could facilitate widely accessible and early detection of epilepsy development, without needing expensive label collection or manual feature extraction.

## 5. ÂÆûÊñΩÂª∫ËÆÆ (Implementation Suggestions)
Begin by implementing baseline Mamba and Transformer models on standard weather datasets (ERA5, WeatherBench). Develop the physics-informed parameterization by: (1) Encoding atmospheric conservation laws as constraints in the system matrices through Lagrangian optimization; (2) Designing weather-dependent transition kernels using known atmospheric regime indicators (e.g., NAO, ENSO phases); (3) Implementing stochastic differential equation formulations for the selection mechanism. For ablation studies: compare physics-constrained vs. unconstrained Mamba, analyze contribution of each system matrix parameterization separately, and test different SDE formulations for transition kernels. Benchmark using specialized metrics: regime transition detection accuracy, long-range dependency correlation scores, and physical consistency metrics (energy conservation, mass balance).

---
<!-- ‚ö†Ô∏è Á≥ªÁªüÂàÜÂâ≤Á∫ø (SYSTEM SEPARATOR) - ËØ∑ÂãøÂà†Èô§Êàñ‰øÆÊîπÊ≠§Ë°å‰ª•‰∏äÂÜÖÂÆπ -->

# üü¢ Áî®Êà∑ÂÜ≥Á≠ñÂå∫ (User Decision)

ËØ∑Âú®‰∏ãÊñπÂ°´ÂÜôÊÇ®ÁöÑÂÜ≥ÂÆö„ÄÇ**ÊîØÊåÅ‰∏≠Êñá**ÔºåÁ≥ªÁªü‰ºöËá™Âä®ÁøªËØë„ÄÇ

**ÂÜ≥Á≠ñ (Action)**: [ APPROVE ] 
<!-- ÈÄâÈ°π: APPROVE (ÈÄöËøá), REVISE (‰øÆÊîπ) -->

**ÂèçÈ¶àÊÑèËßÅ (Feedback)**:
<!-- Â¶ÇÊûúÈÄâÊã© REVISEÔºåËØ∑Â°´ÂÜôÂÖ∑‰Ωì‰øÆÊîπÊÑèËßÅ„ÄÇ‰æãÂ¶ÇÔºö"ËØ∑Â¢ûÂä†ÂØπÊØîÂÆûÈ™å..." -->