# üìù Èò∂ÊÆµÂÆ°Êü•: 01_Research_Round_3

## 1. Ê†∏ÂøÉÊÄùË∑Ø (Refined Idea)
> **This research introduces a Physics-Informed State Space Model (PI-SSM) that addresses the mathematical limitations of existing Mamba architectures for multivariate weather forecasting. The core innovation involves: (1) A novel parameterization of the state space matrices (Œî, A, B, C) through physics-constrained optimization, where conservation laws (mass, energy, momentum) are embedded as hard constraints via Lagrangian multipliers, and atmospheric regime transitions are explicitly modeled using weather-dependent stochastic differential equations with Ornstein-Uhlenbeck processes to capture persistent anomalies; (2) A rigorous approximation theory establishing that standard Mamba's linear time-invariant (LTI) assumption leads to unbounded O(TŒµ) error growth in non-stationary atmospheric dynamics, with proofs demonstrating our physics-informed selection mechanism achieves bounded O(Œµ) approximation error; (3) A specialized evaluation protocol using dynamical systems metrics (Lyapunov exponents, regime transition detection rates) that explicitly tests architectural capacity to capture atmospheric regime changes beyond standard forecasting accuracy metrics.**

## 2. ÊêúÁ¥¢ÂÖ≥ÈîÆËØç (Keywords)
- `Physics-Informed State Space Models`
- `Non-Stationary Time Series Forecasting`
- `Atmospheric Regime Transitions`
- `Hard Constraint Embedding`
- `Ornstein-Uhlenbeck Processes`
- `Linear Time-Varying Systems`
- `Dynamical Systems Evaluation`

## 3. Áõ∏ÂÖ≥Â∑•‰ΩúÁªºËø∞ (Related Work)
Current state space model architectures, particularly Mamba and its hybrid variants like Jamba, demonstrate impressive scalability and efficiency but suffer from fundamental mathematical limitations when applied to complex physical systems. The core issue lies in Mamba's linear time-invariant (LTI) system assumption, which is fundamentally incompatible with the non-stationary, regime-switching behavior of atmospheric dynamics. While hybrid architectures like Jamba-1.5 improve throughput and memory efficiency by interleaving Transformer and Mamba layers with mixture-of-experts, they remain purely data-driven without incorporating physical constraints. This approach risks generating predictions that minimize statistical loss functions but violate fundamental conservation laws. The broader time series forecasting literature, including traditional ARIMA models and modern transformer approaches, similarly fails to address the critical need for physically consistent modeling in chaotic systems. The evaluation protocols in these works focus on standard accuracy metrics without considering dynamical systems properties or regime transition detection capabilities, leaving a significant gap for applications requiring physically plausible long-term predictions.

## 4. ÈáçÁÇπËÆ∫Êñá (Top Papers)
| Year | Citations | Title |
| :---: | :---: | --- |
| 2024 | 310 | [Jamba: A Hybrid Transformer-Mamba Language Model](https://www.semanticscholar.org/paper/cbaf689fd9ea9bc939510019d90535d6249b3367) |
| 2023 | 207 | [A Review of ARIMA vs. Machine Learning Approaches for Time Series Forecasting in Data Driven Networks](https://www.semanticscholar.org/paper/1f1899907a1121e9c224a0cd01676e8ac7c03ff2) |
| 2023 | 129 | [Improving position encoding of transformers for multivariate time series classification](https://www.semanticscholar.org/paper/905a401efd96d4a5705f64b67cef486b43e83e20) |
| 2024 | 52 | [Mamba YOLO: A Simple Baseline for Object Detection with State Space Model](https://www.semanticscholar.org/paper/d5799c2c9feba526270d213404ee5b786da64d84) |
| 2024 | 46 | [Jamba-1.5: Hybrid Transformer-Mamba Models at Scale](https://www.semanticscholar.org/paper/2461816fa38373478499266a7301728584333886) |

### ËÆ∫ÊñáÊëòË¶ÅËØ¶ÊÉÖ:
- **Jamba: A Hybrid Transformer-Mamba Language Model**
  - *Summary*: We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.
- **A Review of ARIMA vs. Machine Learning Approaches for Time Series Forecasting in Data Driven Networks**
  - *Summary*: In the broad scientific field of time series forecasting, the ARIMA models and their variants have been widely applied for half a century now due to their mathematical simplicity and flexibility in application. However, with the recent advances in the development and efficient deployment of artificial intelligence models and techniques, the view is rapidly changing, with a shift towards machine and deep learning approaches becoming apparent, even without a complete evaluation of the superiority of the new approach over the classic statistical algorithms. Our work constitutes an extensive review of the published scientific literature regarding the comparison of ARIMA and machine learning algorithms applied to time series forecasting problems, as well as the combination of these two approaches in hybrid statistical-AI models in a wide variety of data applications (finance, health, weather, utilities, and network traffic prediction). Our review has shown that the AI algorithms display better prediction performance in most applications, with a few notable exceptions analyzed in our Discussion and Conclusions sections, while the hybrid statistical-AI models steadily outperform their individual parts, utilizing the best algorithmic features of both worlds.
- **Improving position encoding of transformers for multivariate time series classification**
  - *Summary*: Transformers have demonstrated outstanding performance in many applications of deep learning. When applied to time series data, transformers require effective position encoding to capture the ordering of the time series data. The efficacy of position encoding in time series analysis is not well-studied and remains controversial, e.g., whether it is better to inject absolute position encoding or relative position encoding, or a combination of them. In order to clarify this, we first review existing absolute and relative position encoding methods when applied in time series classification. We then proposed a new absolute position encoding method dedicated to time series data called time Absolute Position Encoding (tAPE). Our new method incorporates the series length and input embedding dimension in absolute position encoding. Additionally, we propose computationally Efficient implementation of Relative Position Encoding (eRPE) to improve generalisability for time series. We then propose a novel multivariate time series classification model combining tAPE/eRPE and convolution-based input encoding named ConvTran to improve the position and data embedding of time series data. The proposed absolute and relative position encoding methods are simple and efficient. They can be easily integrated into transformer blocks and used for downstream tasks such as forecasting, extrinsic regression, and anomaly detection. Extensive experiments on 32 multivariate time-series datasets show that our model is significantly more accurate than state-of-the-art convolution and transformer-based models. Code and models are open-sourced at https://github.com/Navidfoumani/ConvTran .
- **Mamba YOLO: A Simple Baseline for Object Detection with State Space Model**
  - *Summary*: Driven by the rapid development of deep learning technology, the YOLO series has set a new benchmark for real-time object detectors. Additionally, transformer-based structures have emerged as the most powerful solution in the field, greatly extending the model's receptive field and achieving significant performance improvements. However, this improvement comes at a cost, as the quadratic complexity of the self-attentive mechanism increases the computational burden of the model. To address this problem, we introduce a simple yet effective baseline approach called Mamba YOLO. Our contributions are as follows: 1) We propose that the ODMamba backbone introduce a State Space Model (SSM) with linear complexity to address the quadratic complexity of self-attention. Unlike the other Transformer-base and SSM-base method, ODMamba is simple to train without pretraining. 2) For real-time requirement, we designed the macro structure of ODMamba, determined the optimal stage ratio and scaling size. 3) We design the RG Block that employs a multi-branch structure to model the channel dimensions, which addresses the possible limitations of SSM in sequence modeling, such as insufficient receptive fields and weak image localization. This design captures localized image dependencies more accurately and significantly. Extensive experiments on the publicly available COCO benchmark dataset show that Mamba YOLO achieves state-of-the-art performance compared to previous methods. Specifically, a tiny version of Mamba YOLO achieves a 7.5% improvement in mAP on a single 4090 GPU with an inference time of 1.5 ms.
- **Jamba-1.5: Hybrid Transformer-Mamba Models at Scale**
  - *Summary*: We present Jamba-1.5, new instruction-tuned large language models based on our Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts architecture, providing high throughput and low memory usage across context lengths, while retaining the same or better quality as Transformer models. We release two model sizes: Jamba-1.5-Large, with 94B active parameters, and Jamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a variety of conversational and instruction-following capabilties, and have an effective context length of 256K tokens, the largest amongst open-weight models. To support cost-effective inference, we introduce ExpertsInt8, a novel quantization technique that allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality. When evaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models achieve excellent results while providing high throughput and outperforming other open-weight models on long-context benchmarks. The model weights for both sizes are publicly available under the Jamba Open Model License and we release ExpertsInt8 as open source.

## 5. ÂÆûÊñΩÂª∫ËÆÆ (Implementation Suggestions)
Implement an ablation study comparing: (1) Baseline Mamba with LTI assumption; (2) Physics-informed Mamba with hard constraints via Lagrangian multipliers; (3) Hybrid approach with soft physics regularization. Specifically test: parameterizing A(t) as a function of atmospheric variables (pressure, temperature gradients) using learned basis functions; implementing the Ornstein-Uhlenbeck process for anomaly persistence modeling with tunable mean-reversion parameters; validating constraint satisfaction by monitoring conservation law violations during training. Use ERA5 reanalysis data with explicit regime labels (blocking events, MJO phases) to quantitatively evaluate transition detection rates. Compare error growth rates (O(Œµ) vs O(TŒµ)) on extended forecast horizons to validate the approximation theory.

---
<!-- ‚ö†Ô∏è Á≥ªÁªüÂàÜÂâ≤Á∫ø (SYSTEM SEPARATOR) - ËØ∑ÂãøÂà†Èô§Êàñ‰øÆÊîπÊ≠§Ë°å‰ª•‰∏äÂÜÖÂÆπ -->

# üü¢ Áî®Êà∑ÂÜ≥Á≠ñÂå∫ (User Decision)

ËØ∑Âú®‰∏ãÊñπÂ°´ÂÜôÊÇ®ÁöÑÂÜ≥ÂÆö„ÄÇ**ÊîØÊåÅ‰∏≠Êñá**ÔºåÁ≥ªÁªü‰ºöËá™Âä®ÁøªËØë„ÄÇ

**ÂÜ≥Á≠ñ (Action)**: [ APPROVE ] 
<!-- ÈÄâÈ°π: APPROVE (ÈÄöËøá), REVISE (‰øÆÊîπ) -->

**ÂèçÈ¶àÊÑèËßÅ (Feedback)**:
<!-- Â¶ÇÊûúÈÄâÊã© REVISEÔºåËØ∑Â°´ÂÜôÂÖ∑‰Ωì‰øÆÊîπÊÑèËßÅ„ÄÇ‰æãÂ¶ÇÔºö"ËØ∑Â¢ûÂä†ÂØπÊØîÂÆûÈ™å..." -->