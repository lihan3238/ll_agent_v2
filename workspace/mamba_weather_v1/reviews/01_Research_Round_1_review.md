# üìù Èò∂ÊÆµÂÆ°Êü•: 01_Research_Round_1

## 1. Ê†∏ÂøÉÊÄùË∑Ø (Refined Idea)
> **This research introduces a novel multivariate time series forecasting architecture that integrates Mamba's selective state space models with specialized mechanisms for non-stationary temporal dynamics. The proposed framework explicitly addresses the limitations of existing hybrid approaches by developing a multi-scale decomposition strategy that interfaces with Mamba's selective scanning mechanism, enabling efficient capture of long-range dependencies while adaptively modeling the time-varying statistical properties inherent in weather data through integrated stationarization and normalization modules.**

## 2. ÊêúÁ¥¢ÂÖ≥ÈîÆËØç (Keywords)
- `Selective State Space Models`
- `Non-stationary Time Series Forecasting`
- `Multi-scale Temporal Decomposition`
- `Weather Data Dynamics`
- `Long-range Dependency Modeling`
- `Mamba-Transformer Hybrid Architecture`
- `Adaptive Normalization Mechanisms`

## 3. Áõ∏ÂÖ≥Â∑•‰ΩúÁªºËø∞ (Related Work)
Recent advances in time series forecasting have bifurcated into two dominant paradigms: traditional statistical methods like ARIMA and modern deep learning approaches. While ARIMA models provide mathematical interpretability, they struggle with complex multivariate dependencies inherent in weather systems. Transformer-based models have demonstrated superior performance but face prohibitive computational costs for long-range forecasting due to their quadratic attention complexity. The emergence of state space models, particularly Mamba, offers a promising alternative with linear complexity through selective scanning mechanisms. However, existing hybrid approaches like MAT and SST that combine Mamba with Transformers employ rigid architectural splits that fail to adaptively leverage the strengths of each component based on temporal characteristics. Furthermore, methods like FMamba optimize computational efficiency but overlook the fundamental challenge of non-stationarity in weather data, where statistical properties evolve over time. The critical gap remains in developing architectures that not only efficiently capture long-range dependencies but also explicitly model the multi-scale, non-stationary nature of weather dynamics through adaptive mechanisms that can handle time-varying statistical properties without manual intervention.

## 4. ÈáçÁÇπËÆ∫Êñá (Top Papers)
| Year | Citations | Title |
| :---: | :---: | --- |
| 2023 | 207 | [A Review of ARIMA vs. Machine Learning Approaches for Time Series Forecasting in Data Driven Networks](https://www.semanticscholar.org/paper/1f1899907a1121e9c224a0cd01676e8ac7c03ff2) |
| 2024 | 18 | [Integration of Mamba and Transformer - MAT for Long-Short Range Time Series Forecasting with Application to Weather Dynamics](https://www.semanticscholar.org/paper/3e0aa05e9c0ee4fc6b3c67887960a9ef18c502d4) |
| 2024 | 17 | [SST: Multi-Scale Hybrid Mamba-Transformer Experts for Time Series Forecasting](https://www.semanticscholar.org/paper/4ea7378fc3cb0c6e7092ae162cbd1537ae637597) |
| 2023 | 14 | [Using weather data in energy time series forecasting: the benefit of input data transformations](https://www.semanticscholar.org/paper/1375326bd4e1d6c992550a1d48922977287a32e9) |
| 2024 | 9 | [FMamba: Mamba based on Fast-attention for Multivariate Time-series Forecasting](https://www.semanticscholar.org/paper/3c5ced872aa5d16bf3aa59c40931ed85b54cb3e5) |

### ËÆ∫ÊñáÊëòË¶ÅËØ¶ÊÉÖ:
- **A Review of ARIMA vs. Machine Learning Approaches for Time Series Forecasting in Data Driven Networks**
  - *Summary*: In the broad scientific field of time series forecasting, the ARIMA models and their variants have been widely applied for half a century now due to their mathematical simplicity and flexibility in application. However, with the recent advances in the development and efficient deployment of artificial intelligence models and techniques, the view is rapidly changing, with a shift towards machine and deep learning approaches becoming apparent, even without a complete evaluation of the superiority of the new approach over the classic statistical algorithms. Our work constitutes an extensive review of the published scientific literature regarding the comparison of ARIMA and machine learning algorithms applied to time series forecasting problems, as well as the combination of these two approaches in hybrid statistical-AI models in a wide variety of data applications (finance, health, weather, utilities, and network traffic prediction). Our review has shown that the AI algorithms display better prediction performance in most applications, with a few notable exceptions analyzed in our Discussion and Conclusions sections, while the hybrid statistical-AI models steadily outperform their individual parts, utilizing the best algorithmic features of both worlds.
- **Integration of Mamba and Transformer - MAT for Long-Short Range Time Series Forecasting with Application to Weather Dynamics**
  - *Summary*: Long-short range time series forecasting is essential for predicting future trends and patterns over extended periods. While deep learning models such as Transformers have made significant strides in advancing time series forecasting, they often encounter difficulties in capturing long-term dependencies and effectively managing sparse semantic features. The state space model, Mamba, addresses these issues through its adept handling of selective input and parallel computing, striking a balance between computational efficiency and prediction accuracy. This article examines the advantages and disadvantages of both Mamba and Transformer models, and introduces a combined approach, MAT, which leverages the strengths of each model to capture unique long-short range dependencies and inherent evolutionary patterns in multivariate time series. Specifically, MAT harnesses the long-range dependency capabilities of Mamba and the short-range characteristics of Transformers. Experimental results on benchmark weather datasets demonstrate that MAT outperforms existing comparable methods in terms of prediction accuracy, scalability, and memory efficiency.
- **SST: Multi-Scale Hybrid Mamba-Transformer Experts for Time Series Forecasting**
  - *Summary*: Time series forecasting has made significant advances, including with Transformer-based models. The attention mechanism in Transformer effectively captures temporal dependencies by attending to all past inputs simultaneously. However, its quadratic computational complexity with respect to sequence length limits the scalability for long-range modeling. Recent state space models (SSMs) such as Mamba offer a promising alternative by achieving linear complexity without attention. Yet, Mamba compresses historical information into a fixed-size latent state, potentially causing information loss and limiting representational effectiveness. This raises a key research question: Can we design a hybrid Mamba-Transformer architecture that is both effective and efficient for time series forecasting? To address it, we adapt a hybrid Mamba-Transformer architecture Mambaformer, originally proposed for language modeling, to the time series domain. Preliminary experiments reveal that naively stacking Mamba and Transformer layers in Mambaformer is suboptimal for time series forecasting, due to an information interference problem. To mitigate this issue, we introduce a new time series decomposition strategy that separates time series into long-range patterns and short-range variations. Then we show that Mamba excels at capturing long-term structures, while Transformer is more effective at modeling short-term dynamics. Building on this insight, we propose State Space Transformer (SST), a multi-scale hybrid model with expert modules: a Mamba expert for long-range patterns and a Transformer expert for short-term variations. To facilitate learning the patterns and variations, SST employs a multi-scale patching mechanism to adaptively adjust time series resolution: low resolution for long-term patterns and high resolution for short-term variations. Comprehensive experiments on real-world datasets demonstrate that SST achieves state-of-the-art performance while scaling linearly with sequence length (O(L)). The code is available on GitHub.
- **Using weather data in energy time series forecasting: the benefit of input data transformations**
  - *Summary*: Renewable energy systems depend on the weather, and weather information, thus, plays a crucial role in forecasting time series within such renewable energy systems. However, while weather data are commonly used to improve forecast accuracy, it still has to be determined in which input shape this weather data benefits the forecasting models the most. In the present paper, we investigate how transformations for weather data inputs, i.¬†e., station-based and grid-based weather data, influence the accuracy of energy time series forecasts. The selected weather data transformations are based on statistical features, dimensionality reduction, clustering, autoencoders, and interpolation. We evaluate the performance of these weather data transformations when forecasting three energy time series: electrical demand, solar power, and wind power. Additionally, we compare the best-performing weather data transformations for station-based and grid-based weather data. We show that transforming station-based or grid-based weather data improves the forecast accuracy compared to using the raw weather data between 3.7 and 5.2%, depending on the target energy time series, where statistical and dimensionality reduction data transformations are among the best.
- **FMamba: Mamba based on Fast-attention for Multivariate Time-series Forecasting**
  - *Summary*: In multivariate time-series forecasting (MTSF), extracting the temporal correlations of the input sequences is crucial. While popular Transformer-based predictive models can perform well, their quadratic computational complexity results in inefficiency and high overhead. The recently emerged Mamba, a selective state space model, has shown promising results in many fields due to its strong temporal feature extraction capabilities and linear computational complexity. However, due to the unilateral nature of Mamba, channel-independent predictive models based on Mamba cannot attend to the relationships among all variables in the manner of Transformer-based models. To address this issue, we combine fast-attention with Mamba to introduce a novel framework named FMamba for MTSF. Technically, we first extract the temporal features of the input variables through an embedding layer, then compute the dependencies among input variables via the fast-attention module. Subsequently, we use Mamba to selectively deal with the input features and further extract the temporal dependencies of the variables through the multi-layer perceptron block (MLP-block). Finally, FMamba obtains the predictive results through the projector, a linear layer. Experimental results on eight public datasets demonstrate that FMamba can achieve state-of-the-art performance while maintaining low computational overhead.

## 5. ÂÆûÊñΩÂª∫ËÆÆ (Implementation Suggestions)
Develop a multi-branch architecture with: (1) Adaptive temporal decomposition module that automatically identifies relevant time scales; (2) Learnable routing mechanism that dynamically assigns input segments to Mamba vs Transformer components based on temporal characteristics; (3) Integrated stationarization layers with learnable parameters for handling non-stationarity; (4) Cross-scale attention mechanism to capture interactions between different temporal resolutions; (5) Ablation studies comparing fixed vs adaptive hybrid architectures, different decomposition strategies, and the impact of various normalization schemes on forecasting accuracy across multiple weather datasets with varying temporal characteristics.

---
<!-- ‚ö†Ô∏è Á≥ªÁªüÂàÜÂâ≤Á∫ø (SYSTEM SEPARATOR) - ËØ∑ÂãøÂà†Èô§Êàñ‰øÆÊîπÊ≠§Ë°å‰ª•‰∏äÂÜÖÂÆπ -->

# üü¢ Áî®Êà∑ÂÜ≥Á≠ñÂå∫ (User Decision)

ËØ∑Âú®‰∏ãÊñπÂ°´ÂÜôÊÇ®ÁöÑÂÜ≥ÂÆö„ÄÇ**ÊîØÊåÅ‰∏≠Êñá**ÔºåÁ≥ªÁªü‰ºöËá™Âä®ÁøªËØë„ÄÇ

**ÂÜ≥Á≠ñ (Action)**: [ APPROVE ] 
<!-- ÈÄâÈ°π: APPROVE (ÈÄöËøá), REVISE (‰øÆÊîπ) -->

**ÂèçÈ¶àÊÑèËßÅ (Feedback)**:
<!-- Â¶ÇÊûúÈÄâÊã© REVISEÔºåËØ∑Â°´ÂÜôÂÖ∑‰Ωì‰øÆÊîπÊÑèËßÅ„ÄÇ‰æãÂ¶ÇÔºö"ËØ∑Â¢ûÂä†ÂØπÊØîÂÆûÈ™å..." -->