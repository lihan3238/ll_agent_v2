# üìù Èò∂ÊÆµÂÆ°Êü•: 01_Research_Round_3

## 1. Ê†∏ÂøÉÊÄùË∑Ø (Refined Idea)
> **This research introduces a non-stationary selective state space architecture that overcomes the linear time-invariant (LTI) constraint in Mamba models through time-varying parameterization of state transition matrices A(t) using learnable temporal basis functions. The framework incorporates explicit stationarization modules that model time-varying statistical properties through wavelet-based multi-scale decomposition, enabling adaptation to non-stationary temporal patterns. A differentiable spectral gating mechanism dynamically routes temporal patterns to specialized state space experts using attention-weighted spectral embeddings, overcoming static expert allocation limitations. Theoretical analysis demonstrates that this time-varying formulation constitutes a strictly larger function class than LTI state space models while maintaining O(L log L) computational complexity through selective scanning adaptations.**

## 2. ÊêúÁ¥¢ÂÖ≥ÈîÆËØç (Keywords)
- `non-stationary state space models`
- `time-varying parameterization`
- `wavelet-based stationarization`
- `spectral expert routing`
- `selective scanning`
- `temporal basis functions`
- `differentiable gating`

## 3. Áõ∏ÂÖ≥Â∑•‰ΩúÁªºËø∞ (Related Work)
Recent advances in state space models, particularly Mamba architectures, have demonstrated remarkable efficiency in long-sequence modeling with O(L log L) complexity, positioning them as competitive alternatives to Transformer-based approaches for time series forecasting. However, these methods fundamentally operate under the linear time-invariant (LTI) assumption, where state transition matrices remain constant across time. This constraint severely limits their applicability to real-world non-stationary time series where statistical properties evolve over time. Concurrently, research on bilinear state-space models with time-varying delays has shown the importance of adaptive parameterization, though these approaches typically rely on expectation-maximization algorithms that are computationally intensive and lack the efficiency of modern selective scanning. The extension of Mamba to spatial-temporal graphs demonstrates the architecture's flexibility but inherits the core LTI limitation. Transformer-based methods excel at capturing complex temporal dependencies but suffer from quadratic complexity, while traditional ARIMA models require explicit manual stationarization through differencing operations. This creates a critical gap: the need for efficient, adaptive state space models that can automatically handle non-stationarity while maintaining sub-quadratic computational complexity.

## 4. ÈáçÁÇπËÆ∫Êñá (Top Papers)
| Year | Citations | Title |
| :---: | :---: | --- |
| 2023 | 207 | [A Review of ARIMA vs. Machine Learning Approaches for Time Series Forecasting in Data Driven Networks](https://www.semanticscholar.org/paper/1f1899907a1121e9c224a0cd01676e8ac7c03ff2) |
| 2024 | 161 | [Is Mamba Effective for Time Series Forecasting?](https://www.semanticscholar.org/paper/dba88f9a59ad816ce14d93b2c8bfda5917adc196) |
| 2023 | 73 | [Expectation‚Äêmaximization algorithm for bilinear state‚Äêspace models with time‚Äêvarying delays under non‚ÄêGaussian noise](https://www.semanticscholar.org/paper/15a6b937eafeca7586ddaf1b3eab34c45b238358) |
| 2024 | 27 | [SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding](https://www.semanticscholar.org/paper/bb6afe666ffd07d9059ec94cac551c2b1f33f096) |
| 2024 | 26 | [STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model](https://www.semanticscholar.org/paper/46f33203f6f50bbd38d2e3e3eb1aa4f3eb66715d) |

### ËÆ∫ÊñáÊëòË¶ÅËØ¶ÊÉÖ:
- **A Review of ARIMA vs. Machine Learning Approaches for Time Series Forecasting in Data Driven Networks**
  - *Summary*: In the broad scientific field of time series forecasting, the ARIMA models and their variants have been widely applied for half a century now due to their mathematical simplicity and flexibility in application. However, with the recent advances in the development and efficient deployment of artificial intelligence models and techniques, the view is rapidly changing, with a shift towards machine and deep learning approaches becoming apparent, even without a complete evaluation of the superiority of the new approach over the classic statistical algorithms. Our work constitutes an extensive review of the published scientific literature regarding the comparison of ARIMA and machine learning algorithms applied to time series forecasting problems, as well as the combination of these two approaches in hybrid statistical-AI models in a wide variety of data applications (finance, health, weather, utilities, and network traffic prediction). Our review has shown that the AI algorithms display better prediction performance in most applications, with a few notable exceptions analyzed in our Discussion and Conclusions sections, while the hybrid statistical-AI models steadily outperform their individual parts, utilizing the best algorithmic features of both worlds.
- **Is Mamba Effective for Time Series Forecasting?**
  - *Summary*: In the realm of time series forecasting (TSF), it is imperative for models to adeptly discern and distill hidden patterns within historical time series data to forecast future states. Transformer-based models exhibit formidable efficacy in TSF, primarily attributed to their advantage in apprehending these patterns. However, the quadratic complexity of the Transformer leads to low computational efficiency and high costs, which somewhat hinders the deployment of the TSF model in real-world scenarios. Recently, Mamba, a selective state space model, has gained traction due to its ability to process dependencies in sequences while maintaining near-linear complexity. For TSF tasks, these characteristics enable Mamba to comprehend hidden patterns as the Transformer and reduce computational overhead compared to the Transformer. Therefore, we propose a Mamba-based model named Simple-Mamba (S-Mamba) for TSF. Specifically, we tokenize the time points of each variate autonomously via a linear layer. A bidirectional Mamba layer is utilized to extract inter-variate correlations and a Feed-Forward Network is set to learn temporal dependencies. Finally, the generation of forecast outcomes through a linear mapping layer. Experiments on thirteen public datasets prove that S-Mamba maintains low computational overhead and achieves leading performance. Furthermore, we conduct extensive experiments to explore Mamba's potential in TSF tasks. Our code is available at https://github.com/wzhwzhwzh0921/S-D-Mamba.
- **Expectation‚Äêmaximization algorithm for bilinear state‚Äêspace models with time‚Äêvarying delays under non‚ÄêGaussian noise**
  - *Summary*: In this paper, the parameter identification of bilinear state‚Äêspace model (SSM) in the presence of random outliers and time‚Äêvarying delays is investigated. Under the basis of the observable canonical form of the bilinear model, the system output can be written as a regressive form, and a bilinear state observer is applied to estimate the unknown states. To eliminate the influence of outliers and time‚Äêvarying delays on parameter estimation, we employ the Student's t$$ t $$ distribution to deal with the measurement noise and use a first‚Äêorder Markov chain to model the delays. In the framework of expectation‚Äêmaximization (EM) algorithm, the unknown parameters, delays, noise variance, states and transition probability matrix can be estimated iteratively. A numerical simulation and a continuous stirred tank reactor (CSTR) process demonstrate that the proposed algorithm has good immunity against outliers and time‚Äêvarying delays and offers good estimation accuracy for the bilinear SSM.
- **SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding**
  - *Summary*: Temporal video grounding (TVG) is a critical task in video content understanding, requiring precise alignment between video content and natural language instructions. Despite significant advancements, existing methods face challenges in managing confidence bias towards salient objects and capturing long-term dependencies in video sequences. To address these issues, we introduce SpikeMba: a multi-modal spiking saliency mamba for temporal video grounding. Our approach integrates Spiking Neural Networks (SNNs) with state space models (SSMs) to leverage their unique advantages in handling different aspects of the task. Specifically, we use SNNs to develop a spiking saliency detector that generates the proposal set. The detector emits spike signals when the input signal exceeds a predefined threshold, resulting in a dynamic and binary saliency proposal set. To enhance the model's capability to retain and infer contextual information, we introduce relevant slots which learnable tensors that encode prior knowledge. These slots work with the contextual moment reasoner to maintain a balance between preserving contextual information and exploring semantic relevance dynamically. The SSMs facilitate selective information propagation, addressing the challenge of long-term dependency in video content. By combining SNNs for proposal generation and SSMs for effective contextual reasoning, SpikeMba addresses confidence bias and long-term dependencies, thereby significantly enhancing fine-grained multimodal relationship capture. Our experiments demonstrate the effectiveness of SpikeMba, which consistently outperforms state-of-the-art methods across mainstream benchmarks.
- **STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model**
  - *Summary*: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Spatial-Temporal Selective State Space Module (ST-S3M) to precisely focus on the selected STG latent features. Furthermore, to strengthen GNN's ability of modeling STG data under the setting of selective state space models, we propose Kalman Filtering Graph Neural Networks (KFGN) for dynamically integrate and upgrade the STG embeddings from different temporal granularities through a learnable Kalman Filtering statistical theory-based approach. Extensive empirical studies are conducted on three benchmark STG forecasting datasets, demonstrating the performance superiority and computational efficiency of STG-Mamba. It not only surpasses existing state-of-the-art methods in terms of STG forecasting performance, but also effectively alleviate the computational bottleneck of large-scale graph networks in reducing the computational cost of FLOPs and test inference time. The implementation code is available at: \url{https://github.com/LincanLi98/STG-Mamba}.

## 5. ÂÆûÊñΩÂª∫ËÆÆ (Implementation Suggestions)
Begin by implementing a baseline time-varying state space block with learnable Fourier basis functions for A(t) parameterization. Conduct ablation studies comparing: (1) different basis function families (Fourier vs. Chebyshev vs. learned embeddings), (2) stationarization module variants (wavelet decomposition vs. adaptive normalization vs. learned filters), and (3) spectral gating mechanisms (attention-based vs. MLP-based routing). Validate on non-stationary benchmark datasets with explicit regime changes. Implement gradient checkpointing for the selective scan to handle long sequences. Compare computational complexity against standard Mamba and Transformer baselines, measuring both wall-clock time and memory usage across sequence lengths from 1K to 16K tokens.

---
<!-- ‚ö†Ô∏è Á≥ªÁªüÂàÜÂâ≤Á∫ø (SYSTEM SEPARATOR) - ËØ∑ÂãøÂà†Èô§Êàñ‰øÆÊîπÊ≠§Ë°å‰ª•‰∏äÂÜÖÂÆπ -->

# üü¢ Áî®Êà∑ÂÜ≥Á≠ñÂå∫ (User Decision)

ËØ∑Âú®‰∏ãÊñπÂ°´ÂÜôÊÇ®ÁöÑÂÜ≥ÂÆö„ÄÇ**ÊîØÊåÅ‰∏≠Êñá**ÔºåÁ≥ªÁªü‰ºöËá™Âä®ÁøªËØë„ÄÇ

**ÂÜ≥Á≠ñ (Action)**: [ APPROVE ] 
<!-- ÈÄâÈ°π: APPROVE (ÈÄöËøá), REVISE (‰øÆÊîπ) -->

**ÂèçÈ¶àÊÑèËßÅ (Feedback)**:
<!-- Â¶ÇÊûúÈÄâÊã© REVISEÔºåËØ∑Â°´ÂÜôÂÖ∑‰Ωì‰øÆÊîπÊÑèËßÅ„ÄÇ‰æãÂ¶ÇÔºö"ËØ∑Â¢ûÂä†ÂØπÊØîÂÆûÈ™å..." -->