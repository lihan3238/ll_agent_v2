{
  "project_name": "mamba_weather_v1",
  "current_phase": "architect",
  "user_initial_idea": "I want to use Mamba state space models for time series forecasting on weather data, comparing it with Transformer.",
  "refined_idea": "This research introduces a non-stationary selective state space architecture that overcomes the linear time-invariant (LTI) constraint in Mamba models through time-varying parameterization of state transition matrices A(t) using learnable temporal basis functions. The framework incorporates explicit stationarization modules that model time-varying statistical properties through wavelet-based multi-scale decomposition, enabling adaptation to non-stationary temporal patterns. A differentiable spectral gating mechanism dynamically routes temporal patterns to specialized state space experts using attention-weighted spectral embeddings, overcoming static expert allocation limitations. Theoretical analysis demonstrates that this time-varying formulation constitutes a strictly larger function class than LTI state space models while maintaining O(L log L) computational complexity through selective scanning adaptations.",
  "paper_library": {
    "A Review of ARIMA vs. Machine Learning Approaches for Time Series Forecasting in Data Driven Networks": {
      "title": "A Review of ARIMA vs. Machine Learning Approaches for Time Series Forecasting in Data Driven Networks",
      "year": "2023",
      "citations": 207,
      "summary": "In the broad scientific field of time series forecasting, the ARIMA models and their variants have been widely applied for half a century now due to their mathematical simplicity and flexibility in application. However, with the recent advances in the development and efficient deployment of artificial intelligence models and techniques, the view is rapidly changing, with a shift towards machine and deep learning approaches becoming apparent, even without a complete evaluation of the superiority of the new approach over the classic statistical algorithms. Our work constitutes an extensive review of the published scientific literature regarding the comparison of ARIMA and machine learning algorithms applied to time series forecasting problems, as well as the combination of these two approaches in hybrid statistical-AI models in a wide variety of data applications (finance, health, weather, utilities, and network traffic prediction). Our review has shown that the AI algorithms display better prediction performance in most applications, with a few notable exceptions analyzed in our Discussion and Conclusions sections, while the hybrid statistical-AI models steadily outperform their individual parts, utilizing the best algorithmic features of both worlds.",
      "url": "https://www.semanticscholar.org/paper/1f1899907a1121e9c224a0cd01676e8ac7c03ff2"
    },
    "Is Mamba Effective for Time Series Forecasting?": {
      "title": "Is Mamba Effective for Time Series Forecasting?",
      "year": "2024",
      "citations": 161,
      "summary": "In the realm of time series forecasting (TSF), it is imperative for models to adeptly discern and distill hidden patterns within historical time series data to forecast future states. Transformer-based models exhibit formidable efficacy in TSF, primarily attributed to their advantage in apprehending these patterns. However, the quadratic complexity of the Transformer leads to low computational efficiency and high costs, which somewhat hinders the deployment of the TSF model in real-world scenarios. Recently, Mamba, a selective state space model, has gained traction due to its ability to process dependencies in sequences while maintaining near-linear complexity. For TSF tasks, these characteristics enable Mamba to comprehend hidden patterns as the Transformer and reduce computational overhead compared to the Transformer. Therefore, we propose a Mamba-based model named Simple-Mamba (S-Mamba) for TSF. Specifically, we tokenize the time points of each variate autonomously via a linear layer. A bidirectional Mamba layer is utilized to extract inter-variate correlations and a Feed-Forward Network is set to learn temporal dependencies. Finally, the generation of forecast outcomes through a linear mapping layer. Experiments on thirteen public datasets prove that S-Mamba maintains low computational overhead and achieves leading performance. Furthermore, we conduct extensive experiments to explore Mamba's potential in TSF tasks. Our code is available at https://github.com/wzhwzhwzh0921/S-D-Mamba.",
      "url": "https://www.semanticscholar.org/paper/dba88f9a59ad816ce14d93b2c8bfda5917adc196"
    },
    "Expectation‐maximization algorithm for bilinear state‐space models with time‐varying delays under non‐Gaussian noise": {
      "title": "Expectation‐maximization algorithm for bilinear state‐space models with time‐varying delays under non‐Gaussian noise",
      "year": "2023",
      "citations": 73,
      "summary": "In this paper, the parameter identification of bilinear state‐space model (SSM) in the presence of random outliers and time‐varying delays is investigated. Under the basis of the observable canonical form of the bilinear model, the system output can be written as a regressive form, and a bilinear state observer is applied to estimate the unknown states. To eliminate the influence of outliers and time‐varying delays on parameter estimation, we employ the Student's t$$ t $$ distribution to deal with the measurement noise and use a first‐order Markov chain to model the delays. In the framework of expectation‐maximization (EM) algorithm, the unknown parameters, delays, noise variance, states and transition probability matrix can be estimated iteratively. A numerical simulation and a continuous stirred tank reactor (CSTR) process demonstrate that the proposed algorithm has good immunity against outliers and time‐varying delays and offers good estimation accuracy for the bilinear SSM.",
      "url": "https://www.semanticscholar.org/paper/15a6b937eafeca7586ddaf1b3eab34c45b238358"
    },
    "SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding": {
      "title": "SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding",
      "year": "2024",
      "citations": 27,
      "summary": "Temporal video grounding (TVG) is a critical task in video content understanding, requiring precise alignment between video content and natural language instructions. Despite significant advancements, existing methods face challenges in managing confidence bias towards salient objects and capturing long-term dependencies in video sequences. To address these issues, we introduce SpikeMba: a multi-modal spiking saliency mamba for temporal video grounding. Our approach integrates Spiking Neural Networks (SNNs) with state space models (SSMs) to leverage their unique advantages in handling different aspects of the task. Specifically, we use SNNs to develop a spiking saliency detector that generates the proposal set. The detector emits spike signals when the input signal exceeds a predefined threshold, resulting in a dynamic and binary saliency proposal set. To enhance the model's capability to retain and infer contextual information, we introduce relevant slots which learnable tensors that encode prior knowledge. These slots work with the contextual moment reasoner to maintain a balance between preserving contextual information and exploring semantic relevance dynamically. The SSMs facilitate selective information propagation, addressing the challenge of long-term dependency in video content. By combining SNNs for proposal generation and SSMs for effective contextual reasoning, SpikeMba addresses confidence bias and long-term dependencies, thereby significantly enhancing fine-grained multimodal relationship capture. Our experiments demonstrate the effectiveness of SpikeMba, which consistently outperforms state-of-the-art methods across mainstream benchmarks.",
      "url": "https://www.semanticscholar.org/paper/bb6afe666ffd07d9059ec94cac551c2b1f33f096"
    },
    "STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model": {
      "title": "STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model",
      "year": "2024",
      "citations": 26,
      "summary": "Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Spatial-Temporal Selective State Space Module (ST-S3M) to precisely focus on the selected STG latent features. Furthermore, to strengthen GNN's ability of modeling STG data under the setting of selective state space models, we propose Kalman Filtering Graph Neural Networks (KFGN) for dynamically integrate and upgrade the STG embeddings from different temporal granularities through a learnable Kalman Filtering statistical theory-based approach. Extensive empirical studies are conducted on three benchmark STG forecasting datasets, demonstrating the performance superiority and computational efficiency of STG-Mamba. It not only surpasses existing state-of-the-art methods in terms of STG forecasting performance, but also effectively alleviate the computational bottleneck of large-scale graph networks in reducing the computational cost of FLOPs and test inference time. The implementation code is available at: \\url{https://github.com/LincanLi98/STG-Mamba}.",
      "url": "https://www.semanticscholar.org/paper/46f33203f6f50bbd38d2e3e3eb1aa4f3eb66715d"
    }
  },
  "known_gaps": [
    {
      "existing_method": "Transformer-based models",
      "limitation_description": "Quadratic computational complexity with sequence length limits effective modeling of long-range dependencies in weather data",
      "mathematical_root_cause": "Softmax attention mechanism requires O(L²) computations for sequence length L"
    },
    {
      "existing_method": "Standard Mamba models",
      "limitation_description": "Assumes Linear Time-Invariant (LTI) system properties, failing to adapt to non-stationary weather patterns",
      "mathematical_root_cause": "Fixed state transition matrices cannot adapt to input-dependent dynamics in weather systems"
    },
    {
      "existing_method": "Hybrid Mamba-Transformer architectures (MAT, SST)",
      "limitation_description": "Heuristic fusion mechanisms lack theoretical grounding for optimal long-short range dependency capture",
      "mathematical_root_cause": "Ad-hoc architectural combinations without principled mathematical formulation for multi-scale temporal mixing"
    },
    {
      "existing_method": "Channel-mixing Mamba variants (C-Mamba)",
      "limitation_description": "Overlooks the complex spatiotemporal correlations in multivariate weather data",
      "mathematical_root_cause": "Simplified channel correlation modeling fails to capture nonlinear spatiotemporal interactions"
    },
    {
      "existing_method": "Traditional statistical methods (ARIMA)",
      "limitation_description": "Inadequate for capturing complex nonlinear patterns in high-dimensional weather systems",
      "mathematical_root_cause": "Linear assumptions and stationarity requirements violate real weather dynamics"
    },
    {
      "existing_method": "Standard Mamba Architecture",
      "limitation_description": "Assumes linear time-invariant (LTI) system properties, failing to adapt to the non-stationary statistical properties of weather data where temporal patterns evolve over time",
      "mathematical_root_cause": "Constant state transition matrices in SSMs cannot capture time-varying dynamics: dℎ/dt = Aℎ(t) + Bx(t) with fixed A matrix"
    },
    {
      "existing_method": "Transformer-based Forecasting Models",
      "limitation_description": "Suffer from quadratic computational complexity in sequence length, making long-range weather forecasting computationally prohibitive for operational use",
      "mathematical_root_cause": "O(L²) complexity of self-attention mechanism where L is sequence length, limiting practical application to very long weather sequences"
    },
    {
      "existing_method": "Hybrid Mamba-Transformer Architectures (MAT, SST)",
      "limitation_description": "Employ fixed architectural splits between components without adaptive routing based on temporal characteristics, leading to suboptimal resource allocation",
      "mathematical_root_cause": "Static gating mechanisms that don't dynamically adjust information flow based on multi-scale temporal patterns"
    },
    {
      "existing_method": "Conventional Weather Forecasting Approaches",
      "limitation_description": "Fail to effectively integrate multi-scale temporal dynamics from minutes to seasonal patterns within a unified neural framework",
      "mathematical_root_cause": "Lack of hierarchical representation learning that simultaneously captures short-term fluctuations and long-term climate trends"
    },
    {
      "existing_method": "Input Transformation Methods for Weather Data",
      "limitation_description": "Apply uniform preprocessing without consideration of variable-specific non-stationarity and cross-variable dependencies",
      "mathematical_root_cause": "Global normalization schemes that destroy local temporal patterns and inter-variable relationships critical for weather dynamics"
    },
    {
      "existing_method": "Standard Mamba SSM",
      "limitation_description": "Assumes linear time-invariant (LTI) system dynamics, failing to capture time-varying statistical properties inherent in weather and environmental data",
      "mathematical_root_cause": "Fixed state transition matrix A ∈ R^{N×N} that cannot adapt to changing temporal regimes: h'(t) = Ah(t) + Bx(t), y(t) = Ch(t) + Dx(t)"
    },
    {
      "existing_method": "SST Multi-Scale Hybrid Experts",
      "limitation_description": "Static expert allocation based on fixed frequency bands cannot adapt to regime changes or non-stationary temporal patterns",
      "mathematical_root_cause": "Predefined expert routing f_expert(x_t) = ∑_k w_k · E_k(x_t) where w_k are fixed weights determined by manual frequency splitting"
    },
    {
      "existing_method": "MAT Transformer-Mamba Integration",
      "limitation_description": "Heuristic combination of Transformers and Mamba without principled routing between long-short range dependencies",
      "mathematical_root_cause": "Architectural fusion without theoretical guarantees on when to prefer state space models over attention mechanisms for different temporal regimes"
    },
    {
      "existing_method": "Traditional Weather Forecasting Models",
      "limitation_description": "ARIMA and statistical approaches fail to capture complex non-linear dependencies and long-range interactions in weather systems",
      "mathematical_root_cause": "Linear assumptions in ARIMA(p,d,q): ϕ(B)(1-B)^d y_t = θ(B)ε_t cannot model regime switching and complex seasonality patterns"
    },
    {
      "existing_method": "Traditional ARIMA variants",
      "limitation_description": "Limited capacity to model complex non-linear dependencies and multi-scale temporal patterns in high-dimensional multivariate series",
      "mathematical_root_cause": "Linear parametric assumptions and stationarity requirements that don't scale to complex, non-Gaussian, high-dimensional time series"
    },
    {
      "existing_method": "Transformer-based time series models",
      "limitation_description": "Computationally expensive self-attention mechanisms and inability to efficiently capture long-range dependencies without quadratic complexity",
      "mathematical_root_cause": "O(N²) computational complexity in self-attention and lack of inductive bias for temporal continuity modeling"
    },
    {
      "existing_method": "Static hybrid expert architectures",
      "limitation_description": "Fixed expert allocation that cannot adapt to changing temporal regimes and spectral characteristics over time",
      "mathematical_root_cause": "Predefined routing mechanisms without dynamic, data-dependent expert selection based on evolving time series properties"
    },
    {
      "existing_method": "Spatial-temporal graph Mamba variants",
      "limitation_description": "Focus on spatial relationships while inheriting LTI limitations for temporal modeling, failing to capture time-varying dynamics",
      "mathematical_root_cause": "Extension of LTI assumptions to temporal dimension without addressing fundamental non-stationarity in time series evolution"
    },
    {
      "existing_method": "Mamba (Selective State Space Models)",
      "limitation_description": "Assumes linear time-invariant (LTI) systems, fundamentally limiting expressive power for non-stationary time series with time-varying dynamics",
      "mathematical_root_cause": "Constant state transition matrices A fail to capture time-varying system dynamics: x'(t) = Ax(t) + Bu(t) cannot model A(t)x(t) + B(t)u(t)"
    },
    {
      "existing_method": "Standard Mamba Selection Mechanism",
      "limitation_description": "Static parameter projection lacks adaptability to changing temporal regimes and statistical properties",
      "mathematical_root_cause": "Selection mechanism operates on fixed parameter spaces without temporal conditioning: θ_select = f(x_t) but θ remains constant across time"
    },
    {
      "existing_method": "Transformer-based TSF Models",
      "limitation_description": "Quadratic complexity limits scalability for long-range dependencies despite capturing non-stationary patterns",
      "mathematical_root_cause": "O(L²) attention mechanism becomes computationally prohibitive for very long sequences (L >> 1000)"
    },
    {
      "existing_method": "ARIMA and Traditional Statistical Models",
      "limitation_description": "Require manual stationarity transformations and fixed parametric assumptions",
      "mathematical_root_cause": "Predefined differencing operations and fixed model orders cannot adapt to complex, multi-scale non-stationarities"
    },
    {
      "existing_method": "Hybrid SSM-Expert Approaches",
      "limitation_description": "Static expert allocation fails to dynamically route temporal patterns based on spectral characteristics",
      "mathematical_root_cause": "Fixed routing mechanisms lack differentiable adaptation to time-varying frequency content"
    },
    {
      "existing_method": "Mamba models for time series",
      "limitation_description": "Inherent linear time-invariant (LTI) assumption prevents adaptation to non-stationary temporal patterns and time-varying dynamics",
      "mathematical_root_cause": "Fixed state transition matrices A and constant parameterization across time steps, violating the non-stationarity assumption in real-world time series"
    },
    {
      "existing_method": "Transformer-based time series forecasting",
      "limitation_description": "Quadratic computational complexity O(L²) limits scalability to long sequences despite strong pattern recognition capabilities",
      "mathematical_root_cause": "Pairwise attention mechanism requiring computation over all token pairs in the sequence"
    },
    {
      "existing_method": "Static expert allocation in mixture-of-experts",
      "limitation_description": "Fixed expert assignment fails to adapt to evolving temporal patterns and non-stationary signal characteristics",
      "mathematical_root_cause": "Time-invariant gating functions that do not account for spectral evolution of temporal patterns"
    },
    {
      "existing_method": "Traditional ARIMA models",
      "limitation_description": "Requires manual stationarization through differencing and cannot automatically adapt to complex non-stationary patterns",
      "mathematical_root_cause": "Parametric assumptions of constant mean and variance over time, with fixed differencing orders"
    },
    {
      "existing_method": "Spatial-temporal graph Mamba models",
      "limitation_description": "Extends Mamba to graph structures but inherits the fundamental LTI limitation in the temporal dimension",
      "mathematical_root_cause": "Application of selective state spaces to graph domains without addressing the core time-invariance constraint"
    }
  ],
  "research": {
    "refined_idea": "This research introduces a non-stationary selective state space architecture that overcomes the linear time-invariant (LTI) constraint in Mamba models through time-varying parameterization of state transition matrices A(t) using learnable temporal basis functions. The framework incorporates explicit stationarization modules that model time-varying statistical properties through wavelet-based multi-scale decomposition, enabling adaptation to non-stationary temporal patterns. A differentiable spectral gating mechanism dynamically routes temporal patterns to specialized state space experts using attention-weighted spectral embeddings, overcoming static expert allocation limitations. Theoretical analysis demonstrates that this time-varying formulation constitutes a strictly larger function class than LTI state space models while maintaining O(L log L) computational complexity through selective scanning adaptations.",
    "keywords": [
      "non-stationary state space models",
      "time-varying parameterization",
      "wavelet-based stationarization",
      "spectral expert routing",
      "selective scanning",
      "temporal basis functions",
      "differentiable gating"
    ],
    "gap_analysis": [
      {
        "existing_method": "Transformer-based models",
        "limitation_description": "Quadratic computational complexity with sequence length limits effective modeling of long-range dependencies in weather data",
        "mathematical_root_cause": "Softmax attention mechanism requires O(L²) computations for sequence length L"
      },
      {
        "existing_method": "Standard Mamba models",
        "limitation_description": "Assumes Linear Time-Invariant (LTI) system properties, failing to adapt to non-stationary weather patterns",
        "mathematical_root_cause": "Fixed state transition matrices cannot adapt to input-dependent dynamics in weather systems"
      },
      {
        "existing_method": "Hybrid Mamba-Transformer architectures (MAT, SST)",
        "limitation_description": "Heuristic fusion mechanisms lack theoretical grounding for optimal long-short range dependency capture",
        "mathematical_root_cause": "Ad-hoc architectural combinations without principled mathematical formulation for multi-scale temporal mixing"
      },
      {
        "existing_method": "Channel-mixing Mamba variants (C-Mamba)",
        "limitation_description": "Overlooks the complex spatiotemporal correlations in multivariate weather data",
        "mathematical_root_cause": "Simplified channel correlation modeling fails to capture nonlinear spatiotemporal interactions"
      },
      {
        "existing_method": "Traditional statistical methods (ARIMA)",
        "limitation_description": "Inadequate for capturing complex nonlinear patterns in high-dimensional weather systems",
        "mathematical_root_cause": "Linear assumptions and stationarity requirements violate real weather dynamics"
      },
      {
        "existing_method": "Standard Mamba Architecture",
        "limitation_description": "Assumes linear time-invariant (LTI) system properties, failing to adapt to the non-stationary statistical properties of weather data where temporal patterns evolve over time",
        "mathematical_root_cause": "Constant state transition matrices in SSMs cannot capture time-varying dynamics: dℎ/dt = Aℎ(t) + Bx(t) with fixed A matrix"
      },
      {
        "existing_method": "Transformer-based Forecasting Models",
        "limitation_description": "Suffer from quadratic computational complexity in sequence length, making long-range weather forecasting computationally prohibitive for operational use",
        "mathematical_root_cause": "O(L²) complexity of self-attention mechanism where L is sequence length, limiting practical application to very long weather sequences"
      },
      {
        "existing_method": "Hybrid Mamba-Transformer Architectures (MAT, SST)",
        "limitation_description": "Employ fixed architectural splits between components without adaptive routing based on temporal characteristics, leading to suboptimal resource allocation",
        "mathematical_root_cause": "Static gating mechanisms that don't dynamically adjust information flow based on multi-scale temporal patterns"
      },
      {
        "existing_method": "Conventional Weather Forecasting Approaches",
        "limitation_description": "Fail to effectively integrate multi-scale temporal dynamics from minutes to seasonal patterns within a unified neural framework",
        "mathematical_root_cause": "Lack of hierarchical representation learning that simultaneously captures short-term fluctuations and long-term climate trends"
      },
      {
        "existing_method": "Input Transformation Methods for Weather Data",
        "limitation_description": "Apply uniform preprocessing without consideration of variable-specific non-stationarity and cross-variable dependencies",
        "mathematical_root_cause": "Global normalization schemes that destroy local temporal patterns and inter-variable relationships critical for weather dynamics"
      },
      {
        "existing_method": "Standard Mamba SSM",
        "limitation_description": "Assumes linear time-invariant (LTI) system dynamics, failing to capture time-varying statistical properties inherent in weather and environmental data",
        "mathematical_root_cause": "Fixed state transition matrix A ∈ R^{N×N} that cannot adapt to changing temporal regimes: h'(t) = Ah(t) + Bx(t), y(t) = Ch(t) + Dx(t)"
      },
      {
        "existing_method": "SST Multi-Scale Hybrid Experts",
        "limitation_description": "Static expert allocation based on fixed frequency bands cannot adapt to regime changes or non-stationary temporal patterns",
        "mathematical_root_cause": "Predefined expert routing f_expert(x_t) = ∑_k w_k · E_k(x_t) where w_k are fixed weights determined by manual frequency splitting"
      },
      {
        "existing_method": "MAT Transformer-Mamba Integration",
        "limitation_description": "Heuristic combination of Transformers and Mamba without principled routing between long-short range dependencies",
        "mathematical_root_cause": "Architectural fusion without theoretical guarantees on when to prefer state space models over attention mechanisms for different temporal regimes"
      },
      {
        "existing_method": "Traditional Weather Forecasting Models",
        "limitation_description": "ARIMA and statistical approaches fail to capture complex non-linear dependencies and long-range interactions in weather systems",
        "mathematical_root_cause": "Linear assumptions in ARIMA(p,d,q): ϕ(B)(1-B)^d y_t = θ(B)ε_t cannot model regime switching and complex seasonality patterns"
      },
      {
        "existing_method": "Standard Mamba models",
        "limitation_description": "Assumes linear time-invariant systems, failing to capture time-varying statistical properties and non-stationary patterns in real-world time series",
        "mathematical_root_cause": "Fixed state transition matrices (A) and constant parameterization across time steps, violating the non-stationarity assumption in real-world time series"
      },
      {
        "existing_method": "Traditional ARIMA variants",
        "limitation_description": "Limited capacity to model complex non-linear dependencies and multi-scale temporal patterns in high-dimensional multivariate series",
        "mathematical_root_cause": "Linear parametric assumptions and stationarity requirements that don't scale to complex, non-Gaussian, high-dimensional time series"
      },
      {
        "existing_method": "Transformer-based time series models",
        "limitation_description": "Computationally expensive self-attention mechanisms and inability to efficiently capture long-range dependencies without quadratic complexity",
        "mathematical_root_cause": "O(N²) computational complexity in self-attention and lack of inductive bias for temporal continuity modeling"
      },
      {
        "existing_method": "Static hybrid expert architectures",
        "limitation_description": "Fixed expert allocation that cannot adapt to changing temporal regimes and spectral characteristics over time",
        "mathematical_root_cause": "Predefined routing mechanisms without dynamic, data-dependent expert selection based on evolving time series properties"
      },
      {
        "existing_method": "Spatial-temporal graph Mamba variants",
        "limitation_description": "Focus on spatial relationships while inheriting LTI limitations for temporal modeling, failing to capture time-varying dynamics",
        "mathematical_root_cause": "Extension of LTI assumptions to temporal dimension without addressing fundamental non-stationarity in time series evolution"
      },
      {
        "existing_method": "Mamba (Selective State Space Models)",
        "limitation_description": "Assumes linear time-invariant (LTI) systems, fundamentally limiting expressive power for non-stationary time series with time-varying dynamics",
        "mathematical_root_cause": "Constant state transition matrices A fail to capture time-varying system dynamics: x'(t) = Ax(t) + Bu(t) cannot model A(t)x(t) + B(t)u(t)"
      },
      {
        "existing_method": "Standard Mamba Selection Mechanism",
        "limitation_description": "Static parameter projection lacks adaptability to changing temporal regimes and statistical properties",
        "mathematical_root_cause": "Selection mechanism operates on fixed parameter spaces without temporal conditioning: θ_select = f(x_t) but θ remains constant across time"
      },
      {
        "existing_method": "Transformer-based TSF Models",
        "limitation_description": "Quadratic complexity limits scalability for long-range dependencies despite capturing non-stationary patterns",
        "mathematical_root_cause": "O(L²) attention mechanism becomes computationally prohibitive for very long sequences (L >> 1000)"
      },
      {
        "existing_method": "ARIMA and Traditional Statistical Models",
        "limitation_description": "Require manual stationarity transformations and fixed parametric assumptions",
        "mathematical_root_cause": "Predefined differencing operations and fixed model orders cannot adapt to complex, multi-scale non-stationarities"
      },
      {
        "existing_method": "Hybrid SSM-Expert Approaches",
        "limitation_description": "Static expert allocation fails to dynamically route temporal patterns based on spectral characteristics",
        "mathematical_root_cause": "Fixed routing mechanisms lack differentiable adaptation to time-varying frequency content"
      },
      {
        "existing_method": "Mamba models for time series",
        "limitation_description": "Inherent linear time-invariant (LTI) assumption prevents adaptation to non-stationary temporal patterns and time-varying dynamics",
        "mathematical_root_cause": "Fixed state transition matrices A and constant parameterization across time steps, violating the non-stationarity assumption in real-world time series"
      },
      {
        "existing_method": "Transformer-based time series forecasting",
        "limitation_description": "Quadratic computational complexity O(L²) limits scalability to long sequences despite strong pattern recognition capabilities",
        "mathematical_root_cause": "Pairwise attention mechanism requiring computation over all token pairs in the sequence"
      },
      {
        "existing_method": "Static expert allocation in mixture-of-experts",
        "limitation_description": "Fixed expert assignment fails to adapt to evolving temporal patterns and non-stationary signal characteristics",
        "mathematical_root_cause": "Time-invariant gating functions that do not account for spectral evolution of temporal patterns"
      },
      {
        "existing_method": "Traditional ARIMA models",
        "limitation_description": "Requires manual stationarization through differencing and cannot automatically adapt to complex non-stationary patterns",
        "mathematical_root_cause": "Parametric assumptions of constant mean and variance over time, with fixed differencing orders"
      },
      {
        "existing_method": "Spatial-temporal graph Mamba models",
        "limitation_description": "Extends Mamba to graph structures but inherits the fundamental LTI limitation in the temporal dimension",
        "mathematical_root_cause": "Application of selective state spaces to graph domains without addressing the core time-invariance constraint"
      }
    ],
    "related_work_summary": "Recent advances in state space models, particularly Mamba architectures, have demonstrated remarkable efficiency in long-sequence modeling with O(L log L) complexity, positioning them as competitive alternatives to Transformer-based approaches for time series forecasting. However, these methods fundamentally operate under the linear time-invariant (LTI) assumption, where state transition matrices remain constant across time. This constraint severely limits their applicability to real-world non-stationary time series where statistical properties evolve over time. Concurrently, research on bilinear state-space models with time-varying delays has shown the importance of adaptive parameterization, though these approaches typically rely on expectation-maximization algorithms that are computationally intensive and lack the efficiency of modern selective scanning. The extension of Mamba to spatial-temporal graphs demonstrates the architecture's flexibility but inherits the core LTI limitation. Transformer-based methods excel at capturing complex temporal dependencies but suffer from quadratic complexity, while traditional ARIMA models require explicit manual stationarization through differencing operations. This creates a critical gap: the need for efficient, adaptive state space models that can automatically handle non-stationarity while maintaining sub-quadratic computational complexity.",
    "top_papers": [
      {
        "title": "A Review of ARIMA vs. Machine Learning Approaches for Time Series Forecasting in Data Driven Networks",
        "year": "2023",
        "citations": 207,
        "summary": "In the broad scientific field of time series forecasting, the ARIMA models and their variants have been widely applied for half a century now due to their mathematical simplicity and flexibility in application. However, with the recent advances in the development and efficient deployment of artificial intelligence models and techniques, the view is rapidly changing, with a shift towards machine and deep learning approaches becoming apparent, even without a complete evaluation of the superiority of the new approach over the classic statistical algorithms. Our work constitutes an extensive review of the published scientific literature regarding the comparison of ARIMA and machine learning algorithms applied to time series forecasting problems, as well as the combination of these two approaches in hybrid statistical-AI models in a wide variety of data applications (finance, health, weather, utilities, and network traffic prediction). Our review has shown that the AI algorithms display better prediction performance in most applications, with a few notable exceptions analyzed in our Discussion and Conclusions sections, while the hybrid statistical-AI models steadily outperform their individual parts, utilizing the best algorithmic features of both worlds.",
        "url": "https://www.semanticscholar.org/paper/1f1899907a1121e9c224a0cd01676e8ac7c03ff2"
      },
      {
        "title": "Is Mamba Effective for Time Series Forecasting?",
        "year": "2024",
        "citations": 161,
        "summary": "In the realm of time series forecasting (TSF), it is imperative for models to adeptly discern and distill hidden patterns within historical time series data to forecast future states. Transformer-based models exhibit formidable efficacy in TSF, primarily attributed to their advantage in apprehending these patterns. However, the quadratic complexity of the Transformer leads to low computational efficiency and high costs, which somewhat hinders the deployment of the TSF model in real-world scenarios. Recently, Mamba, a selective state space model, has gained traction due to its ability to process dependencies in sequences while maintaining near-linear complexity. For TSF tasks, these characteristics enable Mamba to comprehend hidden patterns as the Transformer and reduce computational overhead compared to the Transformer. Therefore, we propose a Mamba-based model named Simple-Mamba (S-Mamba) for TSF. Specifically, we tokenize the time points of each variate autonomously via a linear layer. A bidirectional Mamba layer is utilized to extract inter-variate correlations and a Feed-Forward Network is set to learn temporal dependencies. Finally, the generation of forecast outcomes through a linear mapping layer. Experiments on thirteen public datasets prove that S-Mamba maintains low computational overhead and achieves leading performance. Furthermore, we conduct extensive experiments to explore Mamba's potential in TSF tasks. Our code is available at https://github.com/wzhwzhwzh0921/S-D-Mamba.",
        "url": "https://www.semanticscholar.org/paper/dba88f9a59ad816ce14d93b2c8bfda5917adc196"
      },
      {
        "title": "Expectation‐maximization algorithm for bilinear state‐space models with time‐varying delays under non‐Gaussian noise",
        "year": "2023",
        "citations": 73,
        "summary": "In this paper, the parameter identification of bilinear state‐space model (SSM) in the presence of random outliers and time‐varying delays is investigated. Under the basis of the observable canonical form of the bilinear model, the system output can be written as a regressive form, and a bilinear state observer is applied to estimate the unknown states. To eliminate the influence of outliers and time‐varying delays on parameter estimation, we employ the Student's t$$ t $$ distribution to deal with the measurement noise and use a first‐order Markov chain to model the delays. In the framework of expectation‐maximization (EM) algorithm, the unknown parameters, delays, noise variance, states and transition probability matrix can be estimated iteratively. A numerical simulation and a continuous stirred tank reactor (CSTR) process demonstrate that the proposed algorithm has good immunity against outliers and time‐varying delays and offers good estimation accuracy for the bilinear SSM.",
        "url": "https://www.semanticscholar.org/paper/15a6b937eafeca7586ddaf1b3eab34c45b238358"
      },
      {
        "title": "SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding",
        "year": "2024",
        "citations": 27,
        "summary": "Temporal video grounding (TVG) is a critical task in video content understanding, requiring precise alignment between video content and natural language instructions. Despite significant advancements, existing methods face challenges in managing confidence bias towards salient objects and capturing long-term dependencies in video sequences. To address these issues, we introduce SpikeMba: a multi-modal spiking saliency mamba for temporal video grounding. Our approach integrates Spiking Neural Networks (SNNs) with state space models (SSMs) to leverage their unique advantages in handling different aspects of the task. Specifically, we use SNNs to develop a spiking saliency detector that generates the proposal set. The detector emits spike signals when the input signal exceeds a predefined threshold, resulting in a dynamic and binary saliency proposal set. To enhance the model's capability to retain and infer contextual information, we introduce relevant slots which learnable tensors that encode prior knowledge. These slots work with the contextual moment reasoner to maintain a balance between preserving contextual information and exploring semantic relevance dynamically. The SSMs facilitate selective information propagation, addressing the challenge of long-term dependency in video content. By combining SNNs for proposal generation and SSMs for effective contextual reasoning, SpikeMba addresses confidence bias and long-term dependencies, thereby significantly enhancing fine-grained multimodal relationship capture. Our experiments demonstrate the effectiveness of SpikeMba, which consistently outperforms state-of-the-art methods across mainstream benchmarks.",
        "url": "https://www.semanticscholar.org/paper/bb6afe666ffd07d9059ec94cac551c2b1f33f096"
      },
      {
        "title": "STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model",
        "year": "2024",
        "citations": 26,
        "summary": "Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Spatial-Temporal Selective State Space Module (ST-S3M) to precisely focus on the selected STG latent features. Furthermore, to strengthen GNN's ability of modeling STG data under the setting of selective state space models, we propose Kalman Filtering Graph Neural Networks (KFGN) for dynamically integrate and upgrade the STG embeddings from different temporal granularities through a learnable Kalman Filtering statistical theory-based approach. Extensive empirical studies are conducted on three benchmark STG forecasting datasets, demonstrating the performance superiority and computational efficiency of STG-Mamba. It not only surpasses existing state-of-the-art methods in terms of STG forecasting performance, but also effectively alleviate the computational bottleneck of large-scale graph networks in reducing the computational cost of FLOPs and test inference time. The implementation code is available at: \\url{https://github.com/LincanLi98/STG-Mamba}.",
        "url": "https://www.semanticscholar.org/paper/46f33203f6f50bbd38d2e3e3eb1aa4f3eb66715d"
      }
    ],
    "implementation_suggestions": "Begin by implementing a baseline time-varying state space block with learnable Fourier basis functions for A(t) parameterization. Conduct ablation studies comparing: (1) different basis function families (Fourier vs. Chebyshev vs. learned embeddings), (2) stationarization module variants (wavelet decomposition vs. adaptive normalization vs. learned filters), and (3) spectral gating mechanisms (attention-based vs. MLP-based routing). Validate on non-stationary benchmark datasets with explicit regime changes. Implement gradient checkpointing for the selective scan to handle long sequences. Compare computational complexity against standard Mamba and Transformer baselines, measuring both wall-clock time and memory usage across sequence lengths from 1K to 16K tokens."
  },
  "theory": {
    "research_field": "Time Series Forecasting with Non-Stationary State Space Models",
    "problem_formulation": "Given an input multivariate time series $\\mathbf{X} \\in \\mathbb{R}^{L \\times V}$ where $L$ is sequence length and $V$ is number of variates, the goal is to forecast future values $\\mathbf{Y} \\in \\mathbb{R}^{T \\times V}$ under the constraint that real-world time series exhibit non-stationary statistical properties where temporal patterns evolve over time.",
    "proposed_methodology": "We propose **Spectral Adaptive Mamba (SAMba)**, a novel framework that overcomes the linear time-invariant (LTI) constraint through time-varying parameterization of state transition matrices. The core architecture consists of three key components:\n\n1. **Temporal Basis Parameterization**: The state transition matrix $\\mathbf{A}$ becomes time-dependent via learnable basis functions: $\\mathbf{A}(t) = \\sum_{k=1}^{K} \\alpha_k(t) \\mathbf{B}_k$, where $\\{\\mathbf{B}_k\\}_{k=1}^K$ are orthogonal temporal basis matrices and $\\alpha_k(t)$ are input-dependent coefficients computed as $\\alpha_k(t) = \\text{softmax}(\\mathbf{W}_k^\\top \\mathbf{x}_t)$.\n\n2. **Multi-Scale Stationarization Module**: Employ wavelet decomposition to model time-varying statistical properties: $\\mathbf{x}_t = \\sum_{s} \\mathbf{w}_{s,t} + \\epsilon_t$, where $\\mathbf{w}_{s,t}$ represents multi-scale temporal patterns.\n\n3. **Differentiable Spectral Gating**: Dynamic routing of temporal patterns to specialized state space experts using attention-weighted spectral embeddings: $\\mathbf{g}_t = \\text{softmax}(\\mathbf{Q}\\mathbf{K}^\\top / \\sqrt{d})$.\n\n4. **Adaptive Selective Scanning**: Modified scanning mechanism that maintains $O(L \\log L)$ complexity through input-dependent time steps: $\\Delta_t = \\text{softplus}(\\mathbf{W}_\\Delta \\mathbf{x}_t)$.\n\nThe complete forward pass is:\n\n$\\mathbf{h}_t = \\mathbf{A}(t) \\mathbf{h}_{t-1} + \\mathbf{B}(t) \\mathbf{x}_t$\n\n$\\mathbf{y}_t = \\mathbf{C} \\mathbf{h}_t + \\mathbf{D} \\mathbf{x}_t$\n\nwith time-varying parameters enabling adaptation to non-stationary temporal patterns.",
    "theoretical_analysis": "**Theorem 1 (Expressivity)**: SAMba constitutes a strictly larger function class than LTI state space models.\n\n*Proof Sketch*: The time-varying parameterization $\\mathbf{A}(t)$ enables modeling of systems with time-varying dynamics, while standard Mamba models are restricted to LTI systems.\n\n**Theorem 2 (Complexity)**: SAMba maintains $O(L \\log L)$ computational complexity through adaptive selective scanning.\n\n*Proof Sketch*: The discretization step size $\\Delta_t$ becomes input-dependent, preserving the efficiency of selective state space models while gaining adaptivity to non-stationary patterns.\n\n**Complexity Analysis**: The wavelet-based stationarization adds $O(L)$ complexity, while the spectral gating mechanism contributes $O(L \\cdot E)$ where $E$ is number of experts, maintaining overall $O(L \\log L)$ scaling.",
    "key_innovations": [
      "Time-varying state transition matrices via learnable temporal basis functions, breaking the LTI constraint",
      "Explicit multi-scale stationarization through wavelet decomposition for modeling time-varying statistical properties",
      "Differentiable spectral gating for dynamic expert routing based on temporal pattern characteristics",
      "Adaptive selective scanning mechanism that maintains computational efficiency while enabling non-stationary adaptation",
      "Theoretical guarantees on expanded function class while preserving sub-quadratic complexity"
    ]
  },
  "architecture": {
    "project_name": "Spectral Adaptive Mamba (SAMba)",
    "architecture_style": "Non-Stationary State Space Model with Adaptive Parameterization",
    "requirements": [
      "torch>=2.0.0",
      "numpy>=1.21.0",
      "pywavelets>=1.4.0",
      "einops>=0.7.0"
    ],
    "file_structure": [
      {
        "filename": "src\\models\\sam.py",
        "description": "Core SAMba model implementation with time-varying state space parameters",
        "imports": [
          "import torch",
          "import torch.nn as nn",
          "import torch.nn.functional as F",
          "from einops import rearrange, repeat",
          "import math"
        ],
        "classes": [
          {
            "name": "TemporalBasisParameterization",
            "inherits_from": "nn.Module",
            "description": "Time-varying state transition matrix via learnable basis functions",
            "attributes": [
              "self.d_model = config.d_model",
              "self.d_state = config.d_state",
              "self.n_basis = config.n_basis",
              "self.basis_matrices = nn.Parameter(torch.randn(n_basis, d_state, d_state))",
              "self.basis_weights = nn.Linear(d_model, n_basis)",
              "self.orthogonal_constraint = config.orthogonal_constraint"
            ],
            "methods": [
              {
                "name": "__init__",
                "args": [
                  "config: dict"
                ],
                "return_type": "None",
                "docstring": "Initialize temporal basis parameterization",
                "core_logic_steps": [
                  "super().__init__()",
                  "self.d_model = config.d_model",
                  "self.d_state = config.d_state",
                  "self.n_basis = config.n_basis",
                  "self.basis_matrices = nn.Parameter(torch.randn(n_basis, d_state, d_state))",
                  "self.basis_weights = nn.Linear(d_model, n_basis)",
                  "self.orthogonal_constraint = config.orthogonal_constraint"
                ]
              },
              {
                "name": "forward",
                "args": [
                  "x: torch.Tensor"
                ],
                "return_type": "torch.Tensor",
                "docstring": "Compute time-varying A(t) matrices for sequence",
                "core_logic_steps": [
                  "B, L, D = x.shape  # [batch, seq_len, d_model]",
                  "alpha = self.basis_weights(x)  # [B, L, n_basis]",
                  "alpha = F.softmax(alpha, dim=-1)",
                  "basis_matrices = self._apply_orthogonal_constraint(self.basis_matrices)",
                  "A_t = torch.einsum('blk,kij->blij', alpha, basis_matrices)  # [B, L, d_state, d_state]",
                  "return A_t"
                ]
              },
              {
                "name": "_apply_orthogonal_constraint",
                "args": [
                  "matrices: torch.Tensor"
                ],
                "return_type": "torch.Tensor",
                "docstring": "Ensure basis matrices satisfy discretization constraints",
                "core_logic_steps": [
                  "if self.orthogonal_constraint == 'cayley':",
                  "    return self._cayley_transform(matrices)",
                  "elif self.orthogonal_constraint == 'householder':",
                  "    return self._householder_projection(matrices)",
                  "else:",
                  "    return matrices"
                ]
              }
            ]
          },
          {
            "name": "MultiScaleStationarization",
            "inherits_from": "nn.Module",
            "description": "Differentiable wavelet decomposition for non-stationary time series",
            "attributes": [
              "self.d_model = config.d_model",
              "self.n_scales = config.n_scales",
              "self.wavelet_type = config.wavelet_type",
              "self.conv_filters = self._build_wavelet_filters()",
              "self.residual_proj = nn.Linear(d_model * n_scales, d_model)"
            ],
            "methods": [
              {
                "name": "__init__",
                "args": [
                  "config: dict"
                ],
                "return_type": "None",
                "docstring": "Initialize multi-scale stationarization module",
                "core_logic_steps": [
                  "super().__init__()",
                  "self.d_model = config.d_model",
                  "self.n_scales = config.n_scales",
                  "self.wavelet_type = config.wavelet_type",
                  "self.conv_filters = self._build_wavelet_filters()",
                  "self.residual_proj = nn.Linear(d_model * n_scales, d_model)"
                ]
              },
              {
                "name": "forward",
                "args": [
                  "x: torch.Tensor"
                ],
                "return_type": "torch.Tensor",
                "docstring": "Apply multi-scale decomposition and reconstruction",
                "core_logic_steps": [
                  "B, L, D = x.shape  # [batch, seq_len, d_model]",
                  "x_perm = x.permute(0, 2, 1)  # [B, D, L]",
                  "wavelet_coeffs = []",
                  "for scale in range(self.n_scales):",
                  "    filter_length = min(2**(scale+1), L)",
                  "    conv_filter = self.conv_filters[scale]",
                  "    padding = (filter_length - 1) // 2",
                  "    coeff = F.conv1d(x_perm, conv_filter, padding=padding, groups=D)",
                  "    wavelet_coeffs.append(coeff)",
                  "wavelet_components = torch.stack(wavelet_coeffs, dim=1)  # [B, n_scales, D, L]",
                  "wavelet_components = wavelet_components.permute(0, 3, 1, 2)  # [B, L, n_scales, D]",
                  "reconstructed = self.residual_proj(wavelet_components.reshape(B, L, -1))",
                  "return reconstructed"
                ]
              }
            ]
          },
          {
            "name": "LinearSpectralGating",
            "inherits_from": "nn.Module",
            "description": "Linear-time expert routing using spectral embeddings",
            "attributes": [
              "self.d_model = config.d_model",
              "self.n_experts = config.n_experts",
              "self.expert_dim = config.expert_dim",
              "self.query_proj = nn.Linear(d_model, n_experts)",
              "self.key_proj = nn.Linear(d_model, n_experts)",
              "self.value_proj = nn.Linear(d_model, expert_dim * n_experts)",
              "self.output_proj = nn.Linear(expert_dim * n_experts, d_model)"
            ],
            "methods": [
              {
                "name": "__init__",
                "args": [
                  "config: dict"
                ],
                "return_type": "None",
                "docstring": "Initialize linear spectral gating mechanism",
                "core_logic_steps": [
                  "super().__init__()",
                  "self.d_model = config.d_model",
                  "self.n_experts = config.n_experts",
                  "self.expert_dim = config.expert_dim",
                  "self.query_proj = nn.Linear(d_model, n_experts)",
                  "self.key_proj = nn.Linear(d_model, n_experts)",
                  "self.value_proj = nn.Linear(d_model, expert_dim * n_experts)",
                  "self.output_proj = nn.Linear(expert_dim * n_experts, d_model)"
                ]
              },
              {
                "name": "forward",
                "args": [
                  "x: torch.Tensor"
                ],
                "return_type": "torch.Tensor",
                "docstring": "Apply linear-time expert routing",
                "core_logic_steps": [
                  "B, L, D = x.shape  # [batch, seq_len, d_model]",
                  "Q = self.query_proj(x)  # [B, L, n_experts]",
                  "K = self.key_proj(x)  # [B, L, n_experts]",
                  "V = self.value_proj(x)  # [B, L, expert_dim * n_experts]",
                  "gating_weights = torch.einsum('ble,ble->be', Q, K) / math.sqrt(L)  # [B, n_experts]",
                  "gating_weights = F.softmax(gating_weights, dim=-1)",
                  "V_reshaped = V.reshape(B, L, self.n_experts, self.expert_dim)",
                  "expert_output = torch.einsum('be,bleo->blo', gating_weights, V_reshaped)",
                  "expert_output = expert_output.reshape(B, L, -1)",
                  "output = self.output_proj(expert_output)",
                  "return output"
                ]
              }
            ]
          },
          {
            "name": "AdaptiveSelectiveScanning",
            "inherits_from": "nn.Module",
            "description": "Modified selective scanning with input-dependent discretization",
            "attributes": [
              "self.d_model = config.d_model",
              "self.d_state = config.d_state",
              "self.dt_rank = config.dt_rank",
              "self.dt_proj = nn.Linear(dt_rank, d_model)",
              "self.A_parameterization = TemporalBasisParameterization(config)",
              "self.B_proj = nn.Linear(d_model, d_state, bias=False)",
              "self.C_proj = nn.Linear(d_model, d_state, bias=False)",
              "self.D_proj = nn.Linear(d_model, d_model, bias=False)"
            ],
            "methods": [
              {
                "name": "__init__",
                "args": [
                  "config: dict"
                ],
                "return_type": "None",
                "docstring": "Initialize adaptive selective scanning",
                "core_logic_steps": [
                  "super().__init__()",
                  "self.d_model = config.d_model",
                  "self.d_state = config.d_state",
                  "self.dt_rank = config.dt_rank",
                  "self.dt_proj = nn.Linear(dt_rank, d_model)",
                  "self.A_parameterization = TemporalBasisParameterization(config)",
                  "self.B_proj = nn.Linear(d_model, d_state, bias=False)",
                  "self.C_proj = nn.Linear(d_model, d_state, bias=False)",
                  "self.D_proj = nn.Linear(d_model, d_model, bias=False)"
                ]
              },
              {
                "name": "forward",
                "args": [
                  "x: torch.Tensor"
                ],
                "return_type": "torch.Tensor",
                "docstring": "Apply adaptive selective state space scanning",
                "core_logic_steps": [
                  "B, L, D = x.shape  # [batch, seq_len, d_model]",
                  "A_t = self.A_parameterization(x)  # [B, L, d_state, d_state]",
                  "B_t = self.B_proj(x)  # [B, L, d_state]",
                  "C_t = self.C_proj(x)  # [B, L, d_state]",
                  "dt = F.softplus(self.dt_proj(x[:, :, :self.dt_rank]))  # [B, L, d_model]",
                  "h = torch.zeros(B, self.d_state, device=x.device)  # [B, d_state]",
                  "outputs = []",
                  "for t in range(L):",
                  "    A_discrete = torch.matrix_exp(A_t[:, t] * dt[:, t].unsqueeze(-1).unsqueeze(-1))",
                  "    h = torch.einsum('bij,bj->bi', A_discrete, h) + B_t[:, t] * x[:, t]",
                  "    y_t = torch.einsum('bi,bi->b', C_t[:, t], h) + self.D_proj(x[:, t])",
                  "    outputs.append(y_t.unsqueeze(1))",
                  "output = torch.cat(outputs, dim=1)  # [B, L, D]",
                  "return output"
                ]
              }
            ]
          },
          {
            "name": "SAMbaBlock",
            "inherits_from": "nn.Module",
            "description": "Complete SAMba block integrating all components",
            "attributes": [
              "self.d_model = config.d_model",
              "self.norm1 = nn.LayerNorm(d_model)",
              "self.norm2 = nn.LayerNorm(d_model)",
              "self.stationarization = MultiScaleStationarization(config)",
              "self.spectral_gating = LinearSpectralGating(config)",
              "self.selective_scan = AdaptiveSelectiveScanning(config)",
              "self.mlp = nn.Sequential(nn.Linear(d_model, 4*d_model), nn.GELU(), nn.Linear(4*d_model, d_model))"
            ],
            "methods": [
              {
                "name": "__init__",
                "args": [
                  "config: dict"
                ],
                "return_type": "None",
                "docstring": "Initialize complete SAMba block",
                "core_logic_steps": [
                  "super().__init__()",
                  "self.d_model = config.d_model",
                  "self.norm1 = nn.LayerNorm(d_model)",
                  "self.norm2 = nn.LayerNorm(d_model)",
                  "self.stationarization = MultiScaleStationarization(config)",
                  "self.spectral_gating = LinearSpectralGating(config)",
                  "self.selective_scan = AdaptiveSelectiveScanning(config)",
                  "self.mlp = nn.Sequential(nn.Linear(d_model, 4*d_model), nn.GELU(), nn.Linear(4*d_model, d_model))"
                ]
              },
              {
                "name": "forward",
                "args": [
                  "x: torch.Tensor"
                ],
                "return_type": "torch.Tensor",
                "docstring": "Forward pass through SAMba block",
                "core_logic_steps": [
                  "B, L, D = x.shape  # [batch, seq_len, d_model]",
                  "x_stationary = self.stationarization(self.norm1(x))",
                  "x_gated = self.spectral_gating(x_stationary)",
                  "x_ssm = self.selective_scan(x_gated)",
                  "x = x + x_ssm",
                  "x = x + self.mlp(self.norm2(x))",
                  "return x"
                ]
              }
            ]
          },
          {
            "name": "SAMba",
            "inherits_from": "nn.Module",
            "description": "Complete SAMba model for non-stationary time series forecasting",
            "attributes": [
              "self.d_model = config.d_model",
              "self.n_layers = config.n_layers",
              "self.input_proj = nn.Linear(config.input_dim, d_model)",
              "self.output_proj = nn.Linear(d_model, config.output_dim)",
              "self.blocks = nn.ModuleList([SAMbaBlock(config) for _ in range(n_layers)])"
            ],
            "methods": [
              {
                "name": "__init__",
                "args": [
                  "config: dict"
                ],
                "return_type": "None",
                "docstring": "Initialize complete SAMba model",
                "core_logic_steps": [
                  "super().__init__()",
                  "self.d_model = config.d_model",
                  "self.n_layers = config.n_layers",
                  "self.input_proj = nn.Linear(config.input_dim, d_model)",
                  "self.output_proj = nn.Linear(d_model, config.output_dim)",
                  "self.blocks = nn.ModuleList([SAMbaBlock(config) for _ in range(n_layers)])"
                ]
              },
              {
                "name": "forward",
                "args": [
                  "x: torch.Tensor"
                ],
                "return_type": "torch.Tensor",
                "docstring": "Forward pass through complete SAMba model",
                "core_logic_steps": [
                  "B, L, D_in = x.shape  # [batch, seq_len, input_dim]",
                  "x = self.input_proj(x)  # [B, L, d_model]",
                  "for block in self.blocks:",
                  "    x = block(x)",
                  "output = self.output_proj(x)  # [B, L, output_dim]",
                  "return output"
                ]
              }
            ]
          }
        ],
        "functions": []
      },
      {
        "filename": "src\\data\\time_series_loader.py",
        "description": "Data loader for time series forecasting tasks",
        "imports": [
          "import torch",
          "from torch.utils.data import Dataset, DataLoader",
          "import numpy as np"
        ],
        "classes": [
          {
            "name": "TimeSeriesDataset",
            "inherits_from": "Dataset",
            "description": "Dataset for time series forecasting with sliding window",
            "attributes": [
              "self.data = data",
              "self.seq_len = seq_len",
              "self.pred_len = pred_len"
            ],
            "methods": [
              {
                "name": "__init__",
                "args": [
                  "data: torch.Tensor",
                  "seq_len: int",
                  "pred_len: int"
                ],
                "return_type": "None",
                "docstring": "Initialize time series dataset",
                "core_logic_steps": [
                  "super().__init__()",
                  "self.data = data",
                  "self.seq_len = seq_len",
                  "self.pred_len = pred_len"
                ]
              },
              {
                "name": "__len__",
                "args": [],
                "return_type": "int",
                "docstring": "Get dataset length",
                "core_logic_steps": [
                  "return len(self.data) - self.seq_len - self.pred_len + 1"
                ]
              },
              {
                "name": "__getitem__",
                "args": [
                  "idx: int"
                ],
                "return_type": "tuple[torch.Tensor, torch.Tensor]",
                "docstring": "Get input sequence and target",
                "core_logic_steps": [
                  "x = self.data[idx:idx+self.seq_len]  # [seq_len, input_dim]",
                  "y = self.data[idx+self.seq_len:idx+self.seq_len+self.pred_len]  # [pred_len, output_dim]",
                  "return x, y"
                ]
              }
            ]
          }
        ],
        "functions": []
      },
      {
        "filename": "src\\training\\trainer.py",
        "description": "Training loop and evaluation for SAMba model",
        "imports": [
          "import torch",
          "import torch.nn as nn",
          "from torch.optim import AdamW",
          "import numpy as np",
          "from tqdm import tqdm"
        ],
        "classes": [
          {
            "name": "SAMbaTrainer",
            "inherits_from": "nn.Module",
            "description": "Trainer class for SAMba model with validation",
            "attributes": [
              "self.model = model",
              "self.config = config",
              "self.optimizer = AdamW(model.parameters(), lr=config.learning_rate)",
              "self.criterion = nn.MSELoss()"
            ],
            "methods": [
              {
                "name": "__init__",
                "args": [
                  "model: nn.Module",
                  "config: dict"
                ],
                "return_type": "None",
                "docstring": "Initialize SAMba trainer",
                "core_logic_steps": [
                  "super().__init__()",
                  "self.model = model",
                  "self.config = config",
                  "self.optimizer = AdamW(model.parameters(), lr=config.learning_rate)",
                  "self.criterion = nn.MSELoss()"
                ]
              },
              {
                "name": "train_epoch",
                "args": [
                  "train_loader: DataLoader"
                ],
                "return_type": "float",
                "docstring": "Train for one epoch",
                "core_logic_steps": [
                  "self.model.train()",
                  "total_loss = 0",
                  "for batch_idx, (x, y) in enumerate(train_loader):",
                  "    self.optimizer.zero_grad()",
                  "    pred = self.model(x)",
                  "    loss = self.criterion(pred, y)",
                  "    loss.backward()",
                  "    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)",
                  "    self.optimizer.step()",
                  "    total_loss += loss.item()",
                  "return total_loss / len(train_loader)"
                ]
              },
              {
                "name": "validate",
                "args": [
                  "val_loader: DataLoader"
                ],
                "return_type": "float",
                "docstring": "Validate model on validation set",
                "core_logic_steps": [
                  "self.model.eval()",
                  "total_loss = 0",
                  "with torch.no_grad():",
                  "    for x, y in val_loader:",
                  "        pred = self.model(x)",
                  "        loss = self.criterion(pred, y)",
                  "        total_loss += loss.item()",
                  "return total_loss / len(val_loader)"
                ]
              }
            ]
          }
        ],
        "functions": []
      },
      {
        "filename": "src\\tests\\test_components.py",
        "description": "Unit tests for SAMba components",
        "imports": [
          "import torch",
          "import torch.nn as nn",
          "import pytest",
          "from ..models.sam import TemporalBasisParameterization, MultiScaleStationarization, LinearSpectralGating, AdaptiveSelectiveScanning"
        ],
        "classes": [],
        "functions": [
          {
            "name": "test_temporal_basis_gradients",
            "args": [],
            "return_type": "None",
            "docstring": "Test gradient flow through temporal basis parameterization",
            "core_logic_steps": [
              "config = {'d_model': 64, 'd_state': 16, 'n_basis': 8, 'orthogonal_constraint': 'cayley'}",
              "module = TemporalBasisParameterization(config)",
              "x = torch.randn(2, 32, 64, requires_grad=True)",
              "A_t = module(x)",
              "loss = A_t.sum()",
              "loss.backward()",
              "assert x.grad is not None",
              "assert not torch.isnan(x.grad).any()"
            ]
          },
          {
            "name": "test_wavelet_differentiability",
            "args": [],
            "return_type": "None",
            "docstring": "Test differentiability of wavelet decomposition",
            "core_logic_steps": [
              "config = {'d_model': 64, 'n_scales': 4, 'wavelet_type': 'db4'}",
              "module = MultiScaleStationarization(config)",
              "x = torch.randn(2, 32, 64, requires_grad=True)",
              "output = module(x)",
              "loss = output.sum()",
              "loss.backward()",
              "assert x.grad is not None",
              "assert not torch.isnan(x.grad).any()"
            ]
          },
          {
            "name": "test_linear_spectral_gating_complexity",
            "args": [],
            "return_type": "None",
            "docstring": "Verify linear complexity of spectral gating",
            "core_logic_steps": [
              "config = {'d_model': 64, 'n_experts': 4, 'expert_dim': 32}",
              "module = LinearSpectralGating(config)",
              "x = torch.randn(2, 1024, 64)",
              "import time",
              "start = time.time()",
              "output = module(x)",
              "end = time.time()",
              "assert (end - start) < 1.0  # Should be fast for long sequences"
            ]
          }
        ]
      },
      {
        "filename": "configs\\default_config.py",
        "description": "Default configuration for SAMba model",
        "imports": [],
        "classes": [],
        "functions": []
      }
    ],
    "data_flow_diagram": "Input [B, L, D_in] -> Input Projection [B, L, D_model] -> Stationarization [B, L, D_model] -> Spectral Gating [B, L, D_model] -> Adaptive Scanning [B, L, D_model] -> Output Projection [B, L, D_out]",
    "hyperparameters": {
      "d_model": "Hidden dimension size (default: 512)",
      "d_state": "State dimension for SSM (default: 16)",
      "n_basis": "Number of temporal basis functions (default: 8)",
      "n_scales": "Number of wavelet decomposition scales (default: 4)",
      "n_experts": "Number of spectral experts (default: 4)",
      "expert_dim": "Dimension per expert (default: 128)",
      "dt_rank": "Rank for discretization step projection (default: 16)",
      "n_layers": "Number of SAMba blocks (default: 6)",
      "learning_rate": "Initial learning rate (default: 1e-3)",
      "seq_len": "Input sequence length (default: 96)",
      "pred_len": "Prediction horizon length (default: 24)",
      "orthogonal_constraint": "Method for basis matrix constraints ('cayley'|'householder')"
    },
    "main_execution_flow": "1. Load time series data -> 2. Apply multi-scale stationarization -> 3. Route through spectral gating -> 4. Process with adaptive selective scanning -> 5. Output forecast"
  }
}